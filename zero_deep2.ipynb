{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1t6LeUwUVK1Q3QfoOtPbPr-VwuAvqrGIW",
      "authorship_tag": "ABX9TyNh6RfJfL23l476aJkaHfok",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/git-hatano/ds_work/blob/main/zero_deep2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ゼロから作るDeepLearning2\n",
        "https://github.com/oreilly-japan/deep-learning-from-scratch-2"
      ],
      "metadata": {
        "id": "nZbkVT17FHWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import / config"
      ],
      "metadata": {
        "id": "CuPRYnlnO-r3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import collections\n",
        "import pickle\n",
        "import sys"
      ],
      "metadata": {
        "id": "TNgKWpAsaERQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#common/config.py\n",
        "GPU = False\n",
        "\n",
        "#common/np.py\n",
        "if GPU:\n",
        "    import cupy as np\n",
        "    np.cuda.set_allocator(np.cuda.MemoryPool().malloc)\n",
        "\n",
        "    print('\\033[92m' + '-' * 60 + '\\033[0m')\n",
        "    print(' ' * 23 + '\\033[92mGPU Mode (cupy)\\033[0m')\n",
        "    print('\\033[92m' + '-' * 60 + '\\033[0m\\n')\n",
        "else:\n",
        "    import numpy as np"
      ],
      "metadata": {
        "id": "JJrXRqhV6e-G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [dataset](https://github.com/oreilly-japan/deep-learning-from-scratch-2/tree/master/dataset)"
      ],
      "metadata": {
        "id": "aLsnYwRxYX5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Spiral:\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def load_data(self, seed=1984):\n",
        "        np.random.seed(seed)\n",
        "        N = 100  # クラスごとのサンプル数\n",
        "        DIM = 2  # データの要素数\n",
        "        CLS_NUM = 3  # クラス数\n",
        "\n",
        "        x = np.zeros((N*CLS_NUM, DIM))\n",
        "        t = np.zeros((N*CLS_NUM, CLS_NUM), dtype=np.int)\n",
        "\n",
        "        for j in range(CLS_NUM):\n",
        "            for i in range(N):#N*j, N*(j+1)):\n",
        "                rate = i / N\n",
        "                radius = 1.0*rate\n",
        "                theta = j*4.0 + 4.0*rate + np.random.randn()*0.2\n",
        "\n",
        "                ix = N*j + i\n",
        "                x[ix] = np.array([radius*np.sin(theta),\n",
        "                                radius*np.cos(theta)]).flatten()\n",
        "                t[ix, j] = 1\n",
        "\n",
        "        return x, t"
      ],
      "metadata": {
        "id": "k-iIMUPvYXUN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PTB コーパス"
      ],
      "metadata": {
        "id": "xmwYmSk-ajSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import urllib.request\n",
        "except ImportError:\n",
        "    raise ImportError('Use Python3!')\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "url_base = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/'\n",
        "key_file = {\n",
        "    'train':'ptb.train.txt',\n",
        "    'test':'ptb.test.txt',\n",
        "    'valid':'ptb.valid.txt'\n",
        "}\n",
        "save_file = {\n",
        "    'train':'ptb.train.npy',\n",
        "    'test':'ptb.test.npy',\n",
        "    'valid':'ptb.valid.npy'\n",
        "}\n",
        "vocab_file = 'ptb.vocab.pkl'\n",
        "\n",
        "# dataset_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "dataset_dir = \"/content/drive/MyDrive/Colab Notebooks/tmp\"\n",
        "\n",
        "\n",
        "def _download(file_name):\n",
        "    file_path = dataset_dir + '/' + file_name\n",
        "    if os.path.exists(file_path):\n",
        "        return\n",
        "\n",
        "    print('Downloading ' + file_name + ' ... ')\n",
        "\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "    except urllib.error.URLError:\n",
        "        import ssl\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "\n",
        "    print('Done')\n",
        "\n",
        "\n",
        "def load_vocab():\n",
        "    vocab_path = dataset_dir + '/' + vocab_file\n",
        "\n",
        "    if os.path.exists(vocab_path):\n",
        "        with open(vocab_path, 'rb') as f:\n",
        "            word_to_id, id_to_word = pickle.load(f)\n",
        "        return word_to_id, id_to_word\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    data_type = 'train'\n",
        "    file_name = key_file[data_type]\n",
        "    file_path = dataset_dir + '/' + file_name\n",
        "\n",
        "    _download(file_name)\n",
        "\n",
        "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        if word not in word_to_id:\n",
        "            tmp_id = len(word_to_id)\n",
        "            word_to_id[word] = tmp_id\n",
        "            id_to_word[tmp_id] = word\n",
        "\n",
        "    with open(vocab_path, 'wb') as f:\n",
        "        pickle.dump((word_to_id, id_to_word), f)\n",
        "\n",
        "    return word_to_id, id_to_word\n",
        "\n",
        "\n",
        "class Ptb:\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "        \n",
        "    def load_data(self, data_type='train'):\n",
        "        '''\n",
        "            :param data_type: データの種類：'train' or 'test' or 'valid (val)'\n",
        "            :return:\n",
        "        '''\n",
        "        if data_type == 'val': data_type = 'valid'\n",
        "        save_path = dataset_dir + '/' + save_file[data_type]\n",
        "\n",
        "        word_to_id, id_to_word = load_vocab()\n",
        "\n",
        "        if os.path.exists(save_path):\n",
        "            corpus = np.load(save_path)\n",
        "            return corpus, word_to_id, id_to_word\n",
        "\n",
        "        file_name = key_file[data_type]\n",
        "        file_path = dataset_dir + '/' + file_name\n",
        "        _download(file_name)\n",
        "\n",
        "        words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
        "        corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "        np.save(save_path, corpus)\n",
        "        return corpus, word_to_id, id_to_word"
      ],
      "metadata": {
        "id": "Ac08tyKzaby_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [functions](https://github.com/oreilly-japan/deep-learning-from-scratch-2/blob/master/common/functions.py)"
      ],
      "metadata": {
        "id": "67d8pWwEVG1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x - x.max(axis=1, keepdims=True)\n",
        "        x = np.exp(x)\n",
        "        x /= x.sum(axis=1, keepdims=True)\n",
        "    elif x.ndim == 1:\n",
        "        x = x - np.max(x)\n",
        "        x = np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
      ],
      "metadata": {
        "id": "TvNdGo7JVD8S"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [layers](https://github.com/oreilly-japan/deep-learning-from-scratch-2/blob/master/common/layers.py) "
      ],
      "metadata": {
        "id": "dluXuL89RVoS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "skm8MU-IFDlm"
      },
      "outputs": [],
      "source": [
        "class MatMul:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, = self.params\n",
        "        out = np.dot(x, W)\n",
        "        self.x = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        W, = self.params\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dW = np.dot(self.x.T, dout)\n",
        "        self.grads[0][...] = dW\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.params = [W, b]\n",
        "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, b = self.params\n",
        "        out = np.dot(x, W) + b\n",
        "        self.x = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        W, b = self.params\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dW = np.dot(self.x.T, dout)\n",
        "        db = np.sum(dout, axis=0)\n",
        "\n",
        "        self.grads[0][...] = dW\n",
        "        self.grads[1][...] = db\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.out = softmax(x)\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = self.out * dout\n",
        "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
        "        dx -= self.out * sumdx\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.y = None  # softmaxの出力\n",
        "        self.t = None  # 教師ラベル\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "\n",
        "        # 教師ラベルがone-hotベクトルの場合、正解のインデックスに変換\n",
        "        if self.t.size == self.y.size:\n",
        "            self.t = self.t.argmax(axis=1)\n",
        "\n",
        "        loss = cross_entropy_error(self.y, self.t)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "\n",
        "        dx = self.y.copy()\n",
        "        dx[np.arange(batch_size), self.t] -= 1\n",
        "        dx *= dout\n",
        "        dx = dx / batch_size\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = 1 / (1 + np.exp(-x))\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SigmoidWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.loss = None\n",
        "        self.y = None  # sigmoidの出力\n",
        "        self.t = None  # 教師データ\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = 1 / (1 + np.exp(-x))\n",
        "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        dx = (self.y - self.t) * dout / batch_size\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Embedding:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.idx = None\n",
        "    \n",
        "    def forward(self, idx):\n",
        "        W, = self.params\n",
        "        self.idx = idx\n",
        "        out = W[idx]\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dW, = self.grads\n",
        "        dW[...] = 0\n",
        "        for i, word_id in enumerate(self.idx):\n",
        "            dW[word_id] += dout[i]\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [time_layers](https://github.com/oreilly-japan/deep-learning-from-scratch-2/blob/master/common/time_layers.py)"
      ],
      "metadata": {
        "id": "q3VWv_HFqq_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN:\n",
        "    def __init__(self, Wx, Wh, b):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.cache = None\n",
        "    \n",
        "    def forward(self, x, h_prev):\n",
        "        Wx, Wh, b = self.params\n",
        "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
        "        h_next = np.tanh(t)\n",
        "\n",
        "        self.cache = (x, h_prev, h_next)\n",
        "        return h_next\n",
        "\n",
        "    def backward(self, dh_next):\n",
        "        Wx, Wh, b = self.params\n",
        "        x, h_prev, h_next = self.cache\n",
        "\n",
        "        dt = dh_next * (1 - h_next**2)\n",
        "        db = np.sum(dt, axis=0)\n",
        "        dWh = np.dot(h_prev.T, dt)\n",
        "        dh_prev = np.dot(dt, Wh.T)\n",
        "        dWx = np.dot(x.T, dt)\n",
        "        dx = np.dot(dt, Wx.T)\n",
        "\n",
        "        self.grads[0][...] = dWx\n",
        "        self.grads[1][...] = dWh\n",
        "        self.grads[2][...] = db\n",
        "\n",
        "        return dx, dh_prev\n",
        "\n",
        "\n",
        "class TimeRNN:\n",
        "    def __init__(self, Wx, Wh, b, stateful=False):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.layers = None\n",
        "\n",
        "        self.h, self.dh = None, None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def set_state(self, h):\n",
        "        self.h = h\n",
        "    \n",
        "    def reset_state(self):\n",
        "        self.h = None\n",
        "\n",
        "    def forward(self, xs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, D = xs.shape\n",
        "        D, H = Wx. shape\n",
        "\n",
        "        self.layers = []\n",
        "        hs = np.empty((N, T, H), dtype=\"f\")\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H) , dtype=\"f\")\n",
        "        \n",
        "        for t in range(T):\n",
        "            layer = RNN(*self.params)\n",
        "            self.h = layer.forward(xs[:, t, :], self.h)\n",
        "            hs[:, t, :] = self.h\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return hs\n",
        "    \n",
        "    def backward(self, dhs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, H = dhs.shape\n",
        "        D, H = Wx. shape\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype=\"f\")\n",
        "        dh = 0\n",
        "        grads = [0, 0, 0]\n",
        "        for t in reversed(range(T)):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
        "            dxs[:, t, :] = dx\n",
        "        \n",
        "            for i, grad in enumerate(layer.grads):\n",
        "                grads[i] += grad\n",
        "        \n",
        "        for i, grad in enumerate(grads):\n",
        "            self.grads[i][...] = grad\n",
        "        self.dh = dh\n",
        "\n",
        "        return dxs\n",
        "\n",
        "\n",
        "class TimeEmbedding:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.layers = None\n",
        "        self.W = W\n",
        "\n",
        "    def forward(self, xs):\n",
        "        N, T = xs.shape\n",
        "        V, D = self.W.shape\n",
        "\n",
        "        out = np.empty((N, T, D), dtype='f')\n",
        "        self.layers = []\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = Embedding(self.W)\n",
        "            out[:, t, :] = layer.forward(xs[:, t])\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, T, D = dout.shape\n",
        "\n",
        "        grad = 0\n",
        "        for t in range(T):\n",
        "            layer = self.layers[t]\n",
        "            layer.backward(dout[:, t, :])\n",
        "            grad += layer.grads[0]\n",
        "\n",
        "        self.grads[0][...] = grad\n",
        "        return None\n",
        "\n",
        "\n",
        "class TimeAffine:\n",
        "    def __init__(self, W, b):\n",
        "        self.params = [W, b]\n",
        "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, T, D = x.shape\n",
        "        W, b = self.params\n",
        "\n",
        "        rx = x.reshape(N*T, -1)\n",
        "        out = np.dot(rx, W) + b\n",
        "        self.x = x\n",
        "        return out.reshape(N, T, -1)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        x = self.x\n",
        "        N, T, D = x.shape\n",
        "        W, b = self.params\n",
        "\n",
        "        dout = dout.reshape(N*T, -1)\n",
        "        rx = x.reshape(N*T, -1)\n",
        "\n",
        "        db = np.sum(dout, axis=0)\n",
        "        dW = np.dot(rx.T, dout)\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dx = dx.reshape(*x.shape)\n",
        "\n",
        "        self.grads[0][...] = dW\n",
        "        self.grads[1][...] = db\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class TimeSoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.cache = None\n",
        "        self.ignore_label = -1\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        N, T, V = xs.shape\n",
        "\n",
        "        if ts.ndim == 3:  # 教師ラベルがone-hotベクトルの場合\n",
        "            ts = ts.argmax(axis=2)\n",
        "\n",
        "        mask = (ts != self.ignore_label)\n",
        "\n",
        "        # バッチ分と時系列分をまとめる（reshape）\n",
        "        xs = xs.reshape(N * T, V)\n",
        "        ts = ts.reshape(N * T)\n",
        "        mask = mask.reshape(N * T)\n",
        "\n",
        "        ys = softmax(xs)\n",
        "        ls = np.log(ys[np.arange(N * T), ts])\n",
        "        ls *= mask  # ignore_labelに該当するデータは損失を0にする\n",
        "        loss = -np.sum(ls)\n",
        "        loss /= mask.sum()\n",
        "\n",
        "        self.cache = (ts, ys, mask, (N, T, V))\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        ts, ys, mask, (N, T, V) = self.cache\n",
        "\n",
        "        dx = ys\n",
        "        dx[np.arange(N * T), ts] -= 1\n",
        "        dx *= dout\n",
        "        dx /= mask.sum()\n",
        "        dx *= mask[:, np.newaxis]  # ignore_labelに該当するデータは勾配を0にする\n",
        "\n",
        "        dx = dx.reshape((N, T, V))\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class LSTM:\n",
        "    def __init__(self, Wx, Wh, b):\n",
        "        '''\n",
        "        Parameters\n",
        "        ----------\n",
        "        Wx: 入力`x`用の重みパラーメタ（4つ分の重みをまとめる）\n",
        "        Wh: 隠れ状態`h`用の重みパラメータ（4つ分の重みをまとめる）\n",
        "        b: バイアス（4つ分のバイアスをまとめる）\n",
        "        '''\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x, h_prev, c_prev):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, H = h_prev.shape\n",
        "\n",
        "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
        "\n",
        "        f = A[:, :H]\n",
        "        g = A[:, H:2*H]\n",
        "        i = A[:, 2*H:3*H]\n",
        "        o = A[:, 3*H:]\n",
        "\n",
        "        f = sigmoid(f)\n",
        "        g = np.tanh(g)\n",
        "        i = sigmoid(i)\n",
        "        o = sigmoid(o)\n",
        "\n",
        "        c_next = f * c_prev + g * i\n",
        "        h_next = o * np.tanh(c_next)\n",
        "\n",
        "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
        "        return h_next, c_next\n",
        "\n",
        "    def backward(self, dh_next, dc_next):\n",
        "        Wx, Wh, b = self.params\n",
        "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
        "\n",
        "        tanh_c_next = np.tanh(c_next)\n",
        "\n",
        "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
        "\n",
        "        dc_prev = ds * f\n",
        "\n",
        "        di = ds * g\n",
        "        df = ds * c_prev\n",
        "        do = dh_next * tanh_c_next\n",
        "        dg = ds * i\n",
        "\n",
        "        di *= i * (1 - i)\n",
        "        df *= f * (1 - f)\n",
        "        do *= o * (1 - o)\n",
        "        dg *= (1 - g ** 2)\n",
        "\n",
        "        dA = np.hstack((df, dg, di, do))\n",
        "\n",
        "        dWh = np.dot(h_prev.T, dA)\n",
        "        dWx = np.dot(x.T, dA)\n",
        "        db = dA.sum(axis=0)\n",
        "\n",
        "        self.grads[0][...] = dWx\n",
        "        self.grads[1][...] = dWh\n",
        "        self.grads[2][...] = db\n",
        "\n",
        "        dx = np.dot(dA, Wx.T)\n",
        "        dh_prev = np.dot(dA, Wh.T)\n",
        "\n",
        "        return dx, dh_prev, dc_prev\n",
        "\n",
        "\n",
        "class TimeLSTM:\n",
        "    def __init__(self, Wx, Wh, b, stateful=False):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.layers = None\n",
        "\n",
        "        self.h, self.c = None, None\n",
        "        self.dh = None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def forward(self, xs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, D = xs.shape\n",
        "        H = Wh.shape[0]\n",
        "\n",
        "        self.layers = []\n",
        "        hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H), dtype='f')\n",
        "        if not self.stateful or self.c is None:\n",
        "            self.c = np.zeros((N, H), dtype='f')\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = LSTM(*self.params)\n",
        "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
        "            hs[:, t, :] = self.h\n",
        "\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, H = dhs.shape\n",
        "        D = Wx.shape[0]\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        dh, dc = 0, 0\n",
        "\n",
        "        grads = [0, 0, 0]\n",
        "        for t in reversed(range(T)):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
        "            dxs[:, t, :] = dx\n",
        "            for i, grad in enumerate(layer.grads):\n",
        "                grads[i] += grad\n",
        "\n",
        "        for i, grad in enumerate(grads):\n",
        "            self.grads[i][...] = grad\n",
        "        self.dh = dh\n",
        "        return dxs\n",
        "\n",
        "    def set_state(self, h, c=None):\n",
        "        self.h, self.c = h, c\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h, self.c = None, None\n",
        "\n",
        "\n",
        "class TimeDropout:\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.params, self.grads = [], []\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "        self.train_flg = True\n",
        "\n",
        "    def forward(self, xs):\n",
        "        if self.train_flg:\n",
        "            flg = np.random.rand(*xs.shape) > self.dropout_ratio\n",
        "            scale = 1 / (1.0 - self.dropout_ratio)\n",
        "            self.mask = flg.astype(np.float32) * scale\n",
        "\n",
        "            return xs * self.mask\n",
        "        else:\n",
        "            return xs\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask"
      ],
      "metadata": {
        "id": "iDWqmmh1q10h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [optimizer](https://github.com/oreilly-japan/deep-learning-from-scratch-2/blob/master/common/optimizer.py)"
      ],
      "metadata": {
        "id": "qC5Yfd9CXIkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "    \n",
        "    def update(self, params, grads):\n",
        "        for i in range(len(params)):\n",
        "            params[i] -= self.lr * grads[i]\n",
        "\n",
        "\n",
        "class Adam:\n",
        "    '''\n",
        "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
        "    '''\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = [], []\n",
        "            for param in params:\n",
        "                self.m.append(np.zeros_like(param))\n",
        "                self.v.append(np.zeros_like(param))\n",
        "        \n",
        "        self.iter += 1\n",
        "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
        "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
        "            \n",
        "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)"
      ],
      "metadata": {
        "id": "igkyuuo5XTGx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model"
      ],
      "metadata": {
        "id": "fhVuSo55ahfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerNet:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        I, H, O = input_size, hidden_size, output_size\n",
        "\n",
        "        # 重みとバイアスの初期化\n",
        "        W1 = 0.01 * np.random.randn(I, H)\n",
        "        b1 = np.zeros(H)\n",
        "        W2 = 0.01 * np.random.randn(H, O)\n",
        "        b2 = np.zeros(O)\n",
        "\n",
        "        # レイヤの生成\n",
        "        self.layers = [\n",
        "            Affine(W1, b1),\n",
        "            Sigmoid(),\n",
        "            Affine(W2, b2)\n",
        "        ]\n",
        "        self.loss_layer = SoftmaxWithLoss()\n",
        "\n",
        "        # すべての重みと勾配をリストにまとめる\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        score = self.predict(x)\n",
        "        loss = self.loss_layer.forward(score, t)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        # print(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout"
      ],
      "metadata": {
        "id": "qsG8JsDhHDHD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Basemodel](https://github.com/oreilly-japan/deep-learning-from-scratch-2/blob/master/common/base_model.py)"
      ],
      "metadata": {
        "id": "QEDOH7UKdqtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseModel:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = None, None\n",
        "\n",
        "    def forward(self, *args):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, *args):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def save_params(self, file_name=None):\n",
        "        if file_name is None:\n",
        "            file_name = self.__class__.__name__ + '.pkl'\n",
        "\n",
        "        params = [p.astype(np.float16) for p in self.params]\n",
        "        if GPU:\n",
        "            params = [to_cpu(p) for p in params]\n",
        "\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=None):\n",
        "        if file_name is None:\n",
        "            file_name = self.__class__.__name__ + '.pkl'\n",
        "\n",
        "        if '/' in file_name:\n",
        "            file_name = file_name.replace('/', os.sep)\n",
        "\n",
        "        if not os.path.exists(file_name):\n",
        "            raise IOError('No file: ' + file_name)\n",
        "\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "\n",
        "        params = [p.astype('f') for p in params]\n",
        "        if GPU:\n",
        "            params = [to_gpu(p) for p in params]\n",
        "\n",
        "        for i, param in enumerate(self.params):\n",
        "            param[...] = params[i]"
      ],
      "metadata": {
        "id": "Di5OTSfjdt0f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[util](https://github.com/oreilly-japan/deep-learning-from-scratch-2/blob/master/common/util.py)"
      ],
      "metadata": {
        "id": "zZ6Nna0OT6Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_grads(grads, max_norm):\n",
        "    total_norm = 0\n",
        "    for grad in grads:\n",
        "        total_norm += np.sum(grad ** 2)\n",
        "    total_norm = np.sqrt(total_norm)\n",
        "\n",
        "    rate = max_norm / (total_norm + 1e-6)\n",
        "    if rate < 1:\n",
        "        for grad in grads:\n",
        "            grad *= rate\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('.', ' .')\n",
        "    words = text.split(' ')\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    for word in words:\n",
        "        if word not in word_to_id:\n",
        "            new_id = len(word_to_id)\n",
        "            word_to_id[word] = new_id\n",
        "            id_to_word[new_id] = word\n",
        "\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "    return corpus, word_to_id, id_to_word\n",
        "\n",
        "\n",
        "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
        "    '''共起行列の作成\n",
        "    :param corpus: コーパス（単語IDのリスト）\n",
        "    :param vocab_size:語彙数\n",
        "    :param window_size:ウィンドウサイズ（ウィンドウサイズが1のときは、単語の左右1単語がコンテキスト）\n",
        "    :return: 共起行列\n",
        "    '''\n",
        "    corpus_size = len(corpus)\n",
        "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
        "\n",
        "    for idx, word_id in enumerate(corpus):\n",
        "        for i in range(1, window_size + 1):\n",
        "            left_idx = idx - i\n",
        "            right_idx = idx + i\n",
        "\n",
        "            if left_idx >= 0:\n",
        "                left_word_id = corpus[left_idx]\n",
        "                co_matrix[word_id, left_word_id] += 1\n",
        "\n",
        "            if right_idx < corpus_size:\n",
        "                right_word_id = corpus[right_idx]\n",
        "                co_matrix[word_id, right_word_id] += 1\n",
        "\n",
        "    return co_matrix\n",
        "\n",
        "\n",
        "def cos_similarity(x, y, eps=1e-8):\n",
        "    nx = x / np.sqrt(np.sum(x**2) + eps)\n",
        "    ny = y / np.sqrt(np.sum(y**2) + eps)\n",
        "    return np.dot(nx, ny)\n",
        "\n",
        "\n",
        "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
        "    #クエリを取り出す\n",
        "    if query not in word_to_id:\n",
        "        print(f\"{query} is not found\")\n",
        "        return\n",
        "    \n",
        "    print(f\"[query] {query}\")\n",
        "    query_id = word_to_id[query]\n",
        "    query_vec = word_matrix[query_id]\n",
        "\n",
        "    #コサイン類似度の算出\n",
        "    vocab_size = len(id_to_word)\n",
        "    similarity = np.zeros(vocab_size)\n",
        "    for i in range(vocab_size):\n",
        "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
        "    \n",
        "    #コサイン類似度の結果から、その値を高い順に出力\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if id_to_word[i] == query:\n",
        "            continue\n",
        "        print(f\"{id_to_word[i]}: {similarity[i]}\")\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return\n",
        "\n",
        "\n",
        "def ppmi(C, verbose=False, eps=1e-8):\n",
        "    M = np.zeros_like(C, dtype=np.float32)\n",
        "    N = np.sum(C)\n",
        "    S = np.sum(C, axis=0)\n",
        "    total = C.shape[0] * C.shape[1]\n",
        "    cnt = 0\n",
        "\n",
        "    for i in range(C.shape[0]):\n",
        "        for j in range(C.shape[1]):\n",
        "            pmi = np.log2(C[i, j] * N /(S[j]*S[i]) + eps)\n",
        "            M[i, j] = max(0, pmi)\n",
        "\n",
        "            if verbose:\n",
        "                cnt += 1\n",
        "                if cnt % (total//100 + 1) == 0:\n",
        "                    print('%.1f%% done' % (100*cnt/total))\n",
        "    return M\n",
        "\n",
        "\n",
        "def create_contexts_target(corpus, window_size=1):\n",
        "    target = corpus[window_size:-window_size]\n",
        "    contexts = []\n",
        "\n",
        "    for idx in range(window_size, len(corpus)-window_size):\n",
        "        cs = []\n",
        "        for t in range(-window_size, window_size+1):\n",
        "            if t==0:\n",
        "                continue\n",
        "            cs.append(corpus[idx + t])\n",
        "        contexts.append(cs)\n",
        "    \n",
        "    return np.array(contexts), np.array(target)\n",
        "\n",
        "\n",
        "def convert_one_hot(corpus, vocab_size):\n",
        "    '''one-hot表現への変換\n",
        "    :param corpus: 単語IDのリスト（1次元もしくは2次元のNumPy配列）\n",
        "    :param vocab_size: 語彙数\n",
        "    :return: one-hot表現（2次元もしくは3次元のNumPy配列）\n",
        "    '''\n",
        "    N = corpus.shape[0]\n",
        "\n",
        "    if corpus.ndim == 1:\n",
        "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
        "        for idx, word_id in enumerate(corpus):\n",
        "            one_hot[idx, word_id] = 1\n",
        "\n",
        "    elif corpus.ndim == 2:\n",
        "        C = corpus.shape[1]\n",
        "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
        "        for idx_0, word_ids in enumerate(corpus):\n",
        "            for idx_1, word_id in enumerate(word_ids):\n",
        "                one_hot[idx_0, idx_1, word_id] = 1\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "def to_cpu(x):\n",
        "    import numpy\n",
        "    if type(x) == numpy.ndarray:\n",
        "        return x\n",
        "    return np.asnumpy(x)\n",
        "\n",
        "\n",
        "def to_gpu(x):\n",
        "    import cupy\n",
        "    if type(x) == cupy.ndarray:\n",
        "        return x\n",
        "    return cupy.asarray(x)\n",
        "\n",
        "\n",
        "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
        "    for word in (a, b, c):\n",
        "        if word not in word_to_id:\n",
        "            print('%s is not found' % word)\n",
        "            return\n",
        "\n",
        "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
        "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
        "    query_vec = b_vec - a_vec + c_vec\n",
        "    query_vec = normalize(query_vec)\n",
        "\n",
        "    similarity = np.dot(word_matrix, query_vec)\n",
        "\n",
        "    if answer is not None:\n",
        "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
        "\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if np.isnan(similarity[i]):\n",
        "            continue\n",
        "        if id_to_word[i] in (a, b, c):\n",
        "            continue\n",
        "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
        "\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    if x.ndim == 2:\n",
        "        s = np.sqrt((x * x).sum(1))\n",
        "        x /= s.reshape((s.shape[0], 1))\n",
        "    elif x.ndim == 1:\n",
        "        s = np.sqrt((x * x).sum())\n",
        "        x /= s\n",
        "    return x\n",
        "\n",
        "\n",
        "def clip_grads(grads, max_norm):\n",
        "    total_norm = 0\n",
        "    for grad in grads:\n",
        "        total_norm += np.sum(grad ** 2)\n",
        "    total_norm = np.sqrt(total_norm)\n",
        "\n",
        "    rate = max_norm / (total_norm + 1e-6)\n",
        "    if rate < 1:\n",
        "        for grad in grads:\n",
        "            grad *= rate\n",
        "\n",
        "\n",
        "def eval_perplexity(model, corpus, batch_size=10, time_size=35):\n",
        "    print('evaluating perplexity ...')\n",
        "    corpus_size = len(corpus)\n",
        "    total_loss = 0\n",
        "    max_iters = (corpus_size - 1) // (batch_size * time_size)##ここが0になるのがおかしい\n",
        "    # print(corpus_size-1, batch_size * time_size)\n",
        "    jump = (corpus_size - 1) // batch_size\n",
        "\n",
        "    for iters in range(max_iters):\n",
        "        xs = np.zeros((batch_size, time_size), dtype=np.int32)\n",
        "        ts = np.zeros((batch_size, time_size), dtype=np.int32)\n",
        "        time_offset = iters * time_size\n",
        "        offsets = [time_offset + (i * jump) for i in range(batch_size)]\n",
        "        for t in range(time_size):\n",
        "            for i, offset in enumerate(offsets):\n",
        "                xs[i, t] = corpus[(offset + t) % corpus_size]\n",
        "                ts[i, t] = corpus[(offset + t + 1) % corpus_size]\n",
        "\n",
        "        try:\n",
        "            loss = model.forward(xs, ts, train_flg=False)\n",
        "        except TypeError:\n",
        "            loss = model.forward(xs, ts)\n",
        "        total_loss += loss\n",
        "\n",
        "        sys.stdout.write('\\r%d / %d' % (iters, max_iters))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    print('')\n",
        "    # print(total_loss, max_iters)\n",
        "    ppl = np.exp(total_loss / max_iters)\n",
        "    return ppl"
      ],
      "metadata": {
        "id": "KDSEUQeXUADW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Trainer](https://github.com/oreilly-japan/deep-learning-from-scratch-2/blob/master/common/trainer.py)"
      ],
      "metadata": {
        "id": "pKymqCYlS-qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, optimizer):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_list = []\n",
        "        self.eval_interval = None\n",
        "        self.current_epoch = 0\n",
        "\n",
        "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
        "        data_size = len(x)\n",
        "        max_iters = data_size // batch_size\n",
        "        self.eval_interval = eval_interval\n",
        "        model, optimizer = self.model, self.optimizer\n",
        "        total_loss = 0\n",
        "        loss_count = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        for epoch in range(max_epoch):\n",
        "            # シャッフル\n",
        "            idx = np.random.permutation(np.arange(data_size))\n",
        "            x = x[idx]\n",
        "            t = t[idx]\n",
        "\n",
        "            for iters in range(max_iters):\n",
        "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
        "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
        "\n",
        "                # 勾配を求め、パラメータを更新\n",
        "                loss = model.forward(batch_x, batch_t)\n",
        "                model.backward()\n",
        "                params, grads = remove_duplicate(model.params, model.grads)  # 共有された重みを1つに集約\n",
        "                if max_grad is not None:\n",
        "                    clip_grads(grads, max_grad)\n",
        "                optimizer.update(params, grads)\n",
        "                total_loss += loss\n",
        "                loss_count += 1\n",
        "\n",
        "                # 評価\n",
        "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
        "                    avg_loss = total_loss / loss_count\n",
        "                    elapsed_time = time.time() - start_time\n",
        "                    print('| epoch %d |  iter %d / %d | time %d[s] | loss %.2f'\n",
        "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
        "                    self.loss_list.append(float(avg_loss))\n",
        "                    total_loss, loss_count = 0, 0\n",
        "\n",
        "            self.current_epoch += 1\n",
        "\n",
        "    def plot(self, ylim=None):\n",
        "        x = np.arange(len(self.loss_list))\n",
        "        if ylim is not None:\n",
        "            plt.ylim(*ylim)\n",
        "        plt.plot(x, self.loss_list, label='train')\n",
        "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
        "        plt.ylabel('loss')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def remove_duplicate(params, grads):\n",
        "    '''\n",
        "    パラメータ配列中の重複する重みをひとつに集約し、\n",
        "    その重みに対応する勾配を加算する\n",
        "    '''\n",
        "    params, grads = params[:], grads[:]  # copy list\n",
        "\n",
        "    while True:\n",
        "        find_flg = False\n",
        "        L = len(params)\n",
        "\n",
        "        for i in range(0, L - 1):\n",
        "            for j in range(i + 1, L):\n",
        "                # 重みを共有する場合\n",
        "                if params[i] is params[j]:\n",
        "                    grads[i] += grads[j]  # 勾配の加算\n",
        "                    find_flg = True\n",
        "                    params.pop(j)\n",
        "                    grads.pop(j)\n",
        "                # 転置行列として重みを共有する場合（weight tying）\n",
        "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
        "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
        "                    grads[i] += grads[j].T\n",
        "                    find_flg = True\n",
        "                    params.pop(j)\n",
        "                    grads.pop(j)\n",
        "\n",
        "                if find_flg: break\n",
        "            if find_flg: break\n",
        "\n",
        "        if not find_flg: break\n",
        "\n",
        "    return params, grads\n",
        "\n",
        "\n",
        "class RnnlmTrainer:\n",
        "    def __init__(self, model, optimizer):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.time_idx = None\n",
        "        self.ppl_list = None\n",
        "        self.eval_interval = None\n",
        "        self.current_epoch = 0\n",
        "\n",
        "    def get_batch(self, x, t, batch_size, time_size):\n",
        "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
        "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
        "\n",
        "        data_size = len(x)\n",
        "        jump = data_size // batch_size\n",
        "        offsets = [i * jump for i in range(batch_size)]  # バッチの各サンプルの読み込み開始位置\n",
        "\n",
        "        for time in range(time_size):\n",
        "            for i, offset in enumerate(offsets):\n",
        "                batch_x[i, time] = x[(offset + self.time_idx) % data_size]\n",
        "                batch_t[i, time] = t[(offset + self.time_idx) % data_size]\n",
        "            self.time_idx += 1\n",
        "        return batch_x, batch_t\n",
        "\n",
        "    def fit(self, xs, ts, max_epoch=10, batch_size=20, time_size=35,\n",
        "            max_grad=None, eval_interval=20):\n",
        "        data_size = len(xs)\n",
        "        max_iters = data_size // (batch_size * time_size)\n",
        "        self.time_idx = 0\n",
        "        self.ppl_list = []\n",
        "        self.eval_interval = eval_interval\n",
        "        model, optimizer = self.model, self.optimizer\n",
        "        total_loss = 0\n",
        "        loss_count = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        for epoch in range(max_epoch):\n",
        "            for iters in range(max_iters):\n",
        "                batch_x, batch_t = self.get_batch(xs, ts, batch_size, time_size)\n",
        "\n",
        "                # 勾配を求め、パラメータを更新\n",
        "                loss = model.forward(batch_x, batch_t)\n",
        "                model.backward()\n",
        "                params, grads = remove_duplicate(model.params, model.grads)  # 共有された重みを1つに集約\n",
        "                if max_grad is not None:\n",
        "                    clip_grads(grads, max_grad)\n",
        "                optimizer.update(params, grads)\n",
        "                total_loss += loss\n",
        "                loss_count += 1\n",
        "\n",
        "                # パープレキシティの評価\n",
        "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
        "                    ppl = np.exp(total_loss / loss_count)\n",
        "                    elapsed_time = time.time() - start_time\n",
        "                    print('| epoch %d |  iter %d / %d | time %d[s] | perplexity %.2f'\n",
        "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, ppl))\n",
        "                    self.ppl_list.append(float(ppl))\n",
        "                    total_loss, loss_count = 0, 0\n",
        "\n",
        "            self.current_epoch += 1\n",
        "\n",
        "    def plot(self, ylim=None):\n",
        "        x = np.arange(len(self.ppl_list))\n",
        "        if ylim is not None:\n",
        "            plt.ylim(*ylim)\n",
        "        plt.plot(x, self.ppl_list, label='train')\n",
        "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
        "        plt.ylabel('perplexity')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "EtKpSGZoTIOJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1 ニューラルネットの復習"
      ],
      "metadata": {
        "id": "omU7Hx6vGQvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spiral = Spiral()\n",
        "x, t = spiral.load_data()\n",
        "print('x', x.shape)  # (300, 2)\n",
        "print('t', t.shape)  # (300, 3)\n",
        "\n",
        "# データ点のプロット\n",
        "N = 100\n",
        "CLS_NUM = 3\n",
        "markers = ['o', 'x', '^']\n",
        "for i in range(CLS_NUM):\n",
        "    plt.scatter(x[i*N:(i+1)*N, 0], x[i*N:(i+1)*N, 1], s=40, marker=markers[i])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "0V_peAOzO-Oa",
        "outputId": "f0ba0bff-ec99-41d7-b227-0dc53bfd559d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x (300, 2)\n",
            "t (300, 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ハイパパラメータ\n",
        "max_epoch = 300\n",
        "batch_size = 30\n",
        "hidden_size = 10\n",
        "learning_rate = 1.0\n",
        "\n",
        "#データの読み込み\n",
        "spiral = Spiral()\n",
        "x, t = spiral.load_data()\n",
        "\n",
        "#model\n",
        "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
        "#optimizer\n",
        "optimizer = SGD(lr=learning_rate)\n",
        "\n",
        "data_size= len(x)\n",
        "max_iters = data_size // batch_size\n",
        "total_loss = 0\n",
        "loss_count = 0\n",
        "loss_list = []\n",
        "\n",
        "# for epoch in range(max_epoch):\n",
        "#     #データのシャッフル\n",
        "#     idx = np.random.permutation(data_size)\n",
        "#     x = x[idx]\n",
        "#     t = t[idx]\n",
        "    \n",
        "#     for iters in range(max_iters):\n",
        "#         #バッチデータ生成\n",
        "#         batch_x = x[iters*batch_size : (iters+1)*batch_size]\n",
        "#         batch_t = t[iters*batch_size : (iters+1)*batch_size]\n",
        "#         #勾配を求め、パラメータ更新\n",
        "#         loss = model.forward(batch_x, batch_t)\n",
        "#         model.backward()\n",
        "#         optimizer.update(model.params, model.grads)\n",
        "\n",
        "#         total_loss += loss\n",
        "#         loss_count += 1\n",
        "\n",
        "#         if (iters+1) % 10 ==0:\n",
        "#             avg_loss = total_loss / loss_count\n",
        "#             print(f\"| epoch {epoch+1} | iter {iters+1} / {max_iters} | loss {avg_loss}\")\n",
        "#             loss_list.append(avg_loss)\n",
        "#             total_loss = 0\n",
        "#             loss_count = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44tvr8g2Y-WQ",
        "outputId": "57c1c771-df4f-4748-9b20-b51a2496e8ca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 学習結果のプロット\n",
        "# plt.plot(np.arange(len(loss_list)), loss_list, label='train')\n",
        "# plt.xlabel('iterations (x10)')\n",
        "# plt.ylabel('loss')\n",
        "# plt.show()\n",
        "\n",
        "# # 境界領域のプロット\n",
        "# h = 0.001\n",
        "# x_min, x_max = x[:, 0].min() - .1, x[:, 0].max() + .1\n",
        "# y_min, y_max = x[:, 1].min() - .1, x[:, 1].max() + .1\n",
        "# xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "# X = np.c_[xx.ravel(), yy.ravel()]\n",
        "# score = model.predict(X)\n",
        "# predict_cls = np.argmax(score, axis=1)\n",
        "# Z = predict_cls.reshape(xx.shape)\n",
        "# plt.contourf(xx, yy, Z)\n",
        "# plt.axis('off')\n",
        "\n",
        "# # データ点のプロット\n",
        "# x, t = spiral.load_data()\n",
        "# N = 100\n",
        "# CLS_NUM = 3\n",
        "# markers = ['o', 'x', '^']\n",
        "# for i in range(CLS_NUM):\n",
        "#     plt.scatter(x[i*N:(i+1)*N, 0], x[i*N:(i+1)*N, 1], s=40, marker=markers[i])\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "grrumL5LXIfF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ハイパパラメータ\n",
        "max_epoch = 300\n",
        "batch_size = 30\n",
        "hidden_size = 10\n",
        "learning_rate = 1.0\n",
        "\n",
        "#データの読み込み\n",
        "spiral = Spiral()\n",
        "x, t = spiral.load_data()\n",
        "\n",
        "#model\n",
        "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
        "#optimizer\n",
        "optimizer = SGD(lr=learning_rate)\n",
        "\n",
        "# trainer = Trainer(model, optimizer)\n",
        "# trainer.fit(x, t, max_epoch, batch_size, eval_interval=10)\n",
        "# trainer.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBOFFkzaaAnU",
        "outputId": "67cfd526-8b8c-4ac6-ea94-370c6ac80911"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2 自然言語と単語の分散表現\n",
        "\n",
        "*   シソーラスによる手法\n",
        "    * シソーラス：類義語辞書で単語の関連性を表現\n",
        "    * WordNet\n",
        "*   カウントベースの手法\n",
        "\n"
      ],
      "metadata": {
        "id": "qAJ2iGKYWvzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'You say goodbye and I say hello.'\n",
        "text = text.lower()\n",
        "text = text.replace(\".\", \" .\")\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "89JL9agHV4x4",
        "outputId": "e2af4102-1f68-4a4a-ff49-7ee4ecde805b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'you say goodbye and i say hello .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "単語にidを割り振る"
      ],
      "metadata": {
        "id": "qm8E3DbRufFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = text.split(\" \")\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptcLcZ1mbC_c",
        "outputId": "44c337ad-4c06-4b91-caaa-842782ad154a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_id = {}\n",
        "id_to_word = {}\n",
        "for word in words:\n",
        "    if word not in word_to_id:\n",
        "        new_id = len(word_to_id)\n",
        "        word_to_id[word] = new_id\n",
        "        id_to_word[new_id] = word\n",
        "word_to_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJzA6_MYbIei",
        "outputId": "cd1dd33a-49ed-4c76-fb0f-254a86c4336c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "コーパス：文章を単語のidで表したもの"
      ],
      "metadata": {
        "id": "G0_oujxtuG56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [word_to_id[w] for w in words]\n",
        "corpus = np.array(corpus)\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLZOTWMSb6Nh",
        "outputId": "2e1ea453-a427-4174-fd1f-5adcef222c05"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 1, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "共起行列：ある単語の近くにある単語を示す行列"
      ],
      "metadata": {
        "id": "kjgDkOAXuJC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "\n",
        "#共起行列\n",
        "C = np.array([\n",
        "    [0,1,0,0,0,0,0],\n",
        "    [1,0,1,0,1,1,0],\n",
        "    [0,1,0,1,0,0,0],\n",
        "    [0,0,1,0,1,0,0],\n",
        "    [0,1,0,1,0,0,0],\n",
        "    [0,1,0,0,0,0,1],\n",
        "    [0,0,0,0,0,1,0]\n",
        "], dtype=np.int32)\n",
        "\n",
        "print(C[word_to_id[\"goodbye\"]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xENZSb1cVPr",
        "outputId": "5fd94cef-4bec-4fe8-c58b-293baa12b0ec"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 1 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "コサイン類似度"
      ],
      "metadata": {
        "id": "oFG2wMgxuOUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size= len(word_to_id)\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "\n",
        "c0 = C[word_to_id[\"you\"]]\n",
        "c1 = C[word_to_id[\"i\"]]\n",
        "cos_similarity(c0, c1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqe5cFH4f_WP",
        "outputId": "d9999d02-82b1-4c23-8f66-ca9e8d26d69c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7071067758832467"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size= len(word_to_id)\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "\n",
        "most_similar(\"you\", word_to_id, id_to_word, C, top=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hHEanoeoi8g",
        "outputId": "aa68d823-c9b4-46b8-e57c-f75411c64eaf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[query] you\n",
            "goodbye: 0.7071067758832467\n",
            "i: 0.7071067758832467\n",
            "hello: 0.7071067758832467\n",
            "say: 0.0\n",
            "and: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "相互情報量"
      ],
      "metadata": {
        "id": "n9TTtyvIuzkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size= len(word_to_id)\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "W = ppmi(C)\n",
        "\n",
        "np.set_printoptions(precision=3)\n",
        "print('covariance matrix')\n",
        "print(C)\n",
        "print('-'*50)\n",
        "print('PPMI')\n",
        "print(W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FctnGpeos3YY",
        "outputId": "19251121-6cde-4d5c-e362-fc85bdc2e4f2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "covariance matrix\n",
            "[[0 1 0 0 0 0 0]\n",
            " [1 0 1 0 1 1 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 0 1 0 1 0 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 1 0 0 0 0 1]\n",
            " [0 0 0 0 0 1 0]]\n",
            "--------------------------------------------------\n",
            "PPMI\n",
            "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
            " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
            " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
            " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
            " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
            " [0.    0.807 0.    0.    0.    0.    2.807]\n",
            " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "次元削減：特異値分解 (SVD)"
      ],
      "metadata": {
        "id": "0nzZO8cwvRGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "U, S, D = np.linalg.svd(W)\n",
        "\n",
        "print(C[0]) #共起行列\n",
        "print(W[0]) #PPMI行列\n",
        "print(U[0]) #SVD\n",
        "print(U[0, :2]) #SVDで2次元に削減＝先頭の2列を持って来れば良い"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ9a7I50tHPT",
        "outputId": "e62b58c8-8720-450d-f28d-4ec431d00e15"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 0 0 0]\n",
            "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
            "[-1.110e-16  3.409e-01 -4.163e-16 -1.205e-01 -1.110e-16 -9.323e-01\n",
            " -1.086e-16]\n",
            "[-1.110e-16  3.409e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, word_id in word_to_id.items():\n",
        "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
        "plt.scatter(U[:, 0], U[:, 1], alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "zFXfrl4XxOpm",
        "outputId": "711f22ee-1901-4820-d174-19b676e6b705"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ptb = Ptb()\n",
        "# corpus, word_to_id, id_to_word = ptb.load_data(\"train\")\n",
        "\n",
        "# print('corpus size:', len(corpus))\n",
        "# print('corpus[:30]:', corpus[:30])\n",
        "# print()\n",
        "# print('id_to_word[0]:', id_to_word[0])\n",
        "# print('id_to_word[1]:', id_to_word[1])\n",
        "# print('id_to_word[2]:', id_to_word[2])\n",
        "# print()\n",
        "# print(\"word_to_id['car']:\", word_to_id['car'])\n",
        "# print(\"word_to_id['happy']:\", word_to_id['happy'])\n",
        "# print(\"word_to_id['lexus']:\", word_to_id['lexus'])"
      ],
      "metadata": {
        "id": "Bu6akOuWyfUE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# window_size = 2\n",
        "# wordvec_size = 100\n",
        "\n",
        "# ptb = Ptb()\n",
        "# corpus, word_to_id, id_to_word = ptb.load_data(\"train\")\n",
        "# vocab_size = len(word_to_id)\n",
        "# print(\"counting co-occurrence ... \")\n",
        "# C = create_co_matrix(corpus, vocab_size, window_size)\n",
        "# print(\"caluculating PPMI ...\")\n",
        "# W = ppmi(C, verbose=True)\n",
        "\n",
        "# print(\"caluculating SVD ... \")\n",
        "# try:\n",
        "#     from sklearn.utils.extmath import randomized_svd\n",
        "#     U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5, random_state=None)\n",
        "# except ImportError:\n",
        "#     U, S, V = np.linalg.svd(W)\n",
        "\n",
        "# word_vecs = U[:, :wordvec_size]\n",
        "\n",
        "# querys = [\"you\", \"year\", \"car\", \"toyota\"]\n",
        "# for query in querys:\n",
        "#     most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
      ],
      "metadata": {
        "id": "ITMewcSBb1c-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3 word2vec\n",
        "CBOW\n",
        "\n",
        "skip-gram"
      ],
      "metadata": {
        "id": "r-chXiX7g6MV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
        "W = np.random.randn(7, 3)\n",
        "h = np.dot(c, W)\n",
        "h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIhTysgeeMCY",
        "outputId": "027f3ff9-31dc-4148-c2d7-f6b6fec8e37c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.339, -1.26 ,  0.576]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
        "W = np.random.randn(7, 3)\n",
        "layer = MatMul(W)\n",
        "h = layer.forward(c)\n",
        "h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW_rRu1ujWKt",
        "outputId": "6110c956-e53a-4e1a-82b2-951879b2afec"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.491, -0.446,  1.525]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# サンプルのコンテキストデータ\n",
        "c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
        "c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])\n",
        "\n",
        "# 重みの初期化\n",
        "W_in = np.random.randn(7, 3)\n",
        "W_out = np.random.randn(3, 7)\n",
        "\n",
        "# レイヤの生成\n",
        "in_layer0 = MatMul(W_in)\n",
        "in_layer1 = MatMul(W_in)\n",
        "out_layer = MatMul(W_out)\n",
        "\n",
        "# 純伝播\n",
        "h0 = in_layer0.forward(c0)\n",
        "h1 = in_layer1.forward(c1)\n",
        "h = 0.5 * (h0 + h1)\n",
        "s = out_layer.forward(h)\n",
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HTwjpS1j6ty",
        "outputId": "d72b0ea7-d0f8-43ef-efdd-982012c58a6d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.113, -1.843, -1.254,  1.992,  0.988,  0.02 , -0.072]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBOW"
      ],
      "metadata": {
        "id": "lvXr1CrBkQP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCBOW:\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        V, H = vocab_size, hidden_size\n",
        "    \n",
        "        # 重みの初期化\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype(\"f\")\n",
        "        W_out = 0.01 * np.random.randn(H, V).astype(\"f\")\n",
        "\n",
        "        # レイヤの生成\n",
        "        self.in_layer0 = MatMul(W_in)\n",
        "        self.in_layer1 = MatMul(W_in)\n",
        "        self.out_layer = MatMul(W_out)\n",
        "        self.loss_layer = SoftmaxWithLoss()\n",
        "\n",
        "        # 全ての重みと勾配をリストにする\n",
        "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "        \n",
        "        # メンバ変数に単語の分散表現を設定\n",
        "        self.word_vecs = W_in\n",
        "\n",
        "    def forward(self, contexts, target):\n",
        "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
        "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
        "        h = (h0 + h1) * 0.5\n",
        "        score = self.out_layer.forward(h)\n",
        "        loss = self.loss_layer.forward(score, target)\n",
        "        return loss\n",
        "    \n",
        "    def backward(self, dout=1):\n",
        "        ds = self.loss_layer.backward(dout)\n",
        "        da = self.out_layer.backward(ds)\n",
        "        da *= 0.5\n",
        "        self.in_layer1.backward(da)\n",
        "        self.in_layer0.backward(da)\n",
        "        return None"
      ],
      "metadata": {
        "id": "QXPrNEtBv60o"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 1\n",
        "hidden_size = 5\n",
        "batch_size = 3\n",
        "max_epoch = 1000\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "\n",
        "contexts, target = create_contexts_target(corpus, window_size=1)\n",
        "# print(contexts)\n",
        "# print(target)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "target = convert_one_hot(target, vocab_size)\n",
        "contexts = convert_one_hot(contexts, vocab_size)\n",
        "\n",
        "model = SimpleCBOW(vocab_size, hidden_size)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "trainer.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a-o4FbfvnY4a",
        "outputId": "1fbbb781-2044-497c-b648-40d8cbe17022"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch 1 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 2 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 3 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 4 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 5 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 6 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 7 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 8 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 9 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 10 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 11 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 12 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 13 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 14 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 15 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 16 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 17 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 18 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 19 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 20 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 21 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 22 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 23 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 24 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 25 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 26 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 27 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 28 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 29 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 30 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 31 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 32 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 33 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 34 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 35 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 36 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 37 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 38 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 39 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 40 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
            "| epoch 41 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
            "| epoch 42 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
            "| epoch 43 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
            "| epoch 44 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
            "| epoch 45 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
            "| epoch 46 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
            "| epoch 47 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
            "| epoch 48 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
            "| epoch 49 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
            "| epoch 50 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
            "| epoch 51 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
            "| epoch 52 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
            "| epoch 53 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
            "| epoch 54 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
            "| epoch 55 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
            "| epoch 56 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
            "| epoch 57 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
            "| epoch 58 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
            "| epoch 59 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
            "| epoch 60 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
            "| epoch 61 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
            "| epoch 62 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
            "| epoch 63 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
            "| epoch 64 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
            "| epoch 65 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
            "| epoch 66 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
            "| epoch 67 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
            "| epoch 68 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
            "| epoch 69 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
            "| epoch 70 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
            "| epoch 71 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
            "| epoch 72 |  iter 1 / 2 | time 0[s] | loss 1.81\n",
            "| epoch 73 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
            "| epoch 74 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
            "| epoch 75 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
            "| epoch 76 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
            "| epoch 77 |  iter 1 / 2 | time 0[s] | loss 1.80\n",
            "| epoch 78 |  iter 1 / 2 | time 0[s] | loss 1.80\n",
            "| epoch 79 |  iter 1 / 2 | time 0[s] | loss 1.78\n",
            "| epoch 80 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
            "| epoch 81 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
            "| epoch 82 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
            "| epoch 83 |  iter 1 / 2 | time 0[s] | loss 1.78\n",
            "| epoch 84 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
            "| epoch 85 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
            "| epoch 86 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
            "| epoch 87 |  iter 1 / 2 | time 0[s] | loss 1.75\n",
            "| epoch 88 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
            "| epoch 89 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
            "| epoch 90 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
            "| epoch 91 |  iter 1 / 2 | time 0[s] | loss 1.75\n",
            "| epoch 92 |  iter 1 / 2 | time 0[s] | loss 1.73\n",
            "| epoch 93 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
            "| epoch 94 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
            "| epoch 95 |  iter 1 / 2 | time 0[s] | loss 1.71\n",
            "| epoch 96 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
            "| epoch 97 |  iter 1 / 2 | time 0[s] | loss 1.73\n",
            "| epoch 98 |  iter 1 / 2 | time 0[s] | loss 1.71\n",
            "| epoch 99 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
            "| epoch 100 |  iter 1 / 2 | time 0[s] | loss 1.69\n",
            "| epoch 101 |  iter 1 / 2 | time 0[s] | loss 1.70\n",
            "| epoch 102 |  iter 1 / 2 | time 0[s] | loss 1.68\n",
            "| epoch 103 |  iter 1 / 2 | time 0[s] | loss 1.71\n",
            "| epoch 104 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
            "| epoch 105 |  iter 1 / 2 | time 0[s] | loss 1.70\n",
            "| epoch 106 |  iter 1 / 2 | time 0[s] | loss 1.66\n",
            "| epoch 107 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
            "| epoch 108 |  iter 1 / 2 | time 0[s] | loss 1.68\n",
            "| epoch 109 |  iter 1 / 2 | time 0[s] | loss 1.64\n",
            "| epoch 110 |  iter 1 / 2 | time 0[s] | loss 1.64\n",
            "| epoch 111 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
            "| epoch 112 |  iter 1 / 2 | time 0[s] | loss 1.66\n",
            "| epoch 113 |  iter 1 / 2 | time 0[s] | loss 1.62\n",
            "| epoch 114 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
            "| epoch 115 |  iter 1 / 2 | time 0[s] | loss 1.61\n",
            "| epoch 116 |  iter 1 / 2 | time 0[s] | loss 1.66\n",
            "| epoch 117 |  iter 1 / 2 | time 0[s] | loss 1.61\n",
            "| epoch 118 |  iter 1 / 2 | time 0[s] | loss 1.59\n",
            "| epoch 119 |  iter 1 / 2 | time 0[s] | loss 1.62\n",
            "| epoch 120 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
            "| epoch 121 |  iter 1 / 2 | time 0[s] | loss 1.62\n",
            "| epoch 122 |  iter 1 / 2 | time 0[s] | loss 1.59\n",
            "| epoch 123 |  iter 1 / 2 | time 0[s] | loss 1.57\n",
            "| epoch 124 |  iter 1 / 2 | time 0[s] | loss 1.63\n",
            "| epoch 125 |  iter 1 / 2 | time 0[s] | loss 1.55\n",
            "| epoch 126 |  iter 1 / 2 | time 0[s] | loss 1.59\n",
            "| epoch 127 |  iter 1 / 2 | time 0[s] | loss 1.60\n",
            "| epoch 128 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
            "| epoch 129 |  iter 1 / 2 | time 0[s] | loss 1.58\n",
            "| epoch 130 |  iter 1 / 2 | time 0[s] | loss 1.55\n",
            "| epoch 131 |  iter 1 / 2 | time 0[s] | loss 1.52\n",
            "| epoch 132 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
            "| epoch 133 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
            "| epoch 134 |  iter 1 / 2 | time 0[s] | loss 1.52\n",
            "| epoch 135 |  iter 1 / 2 | time 0[s] | loss 1.53\n",
            "| epoch 136 |  iter 1 / 2 | time 0[s] | loss 1.53\n",
            "| epoch 137 |  iter 1 / 2 | time 0[s] | loss 1.53\n",
            "| epoch 138 |  iter 1 / 2 | time 0[s] | loss 1.53\n",
            "| epoch 139 |  iter 1 / 2 | time 0[s] | loss 1.50\n",
            "| epoch 140 |  iter 1 / 2 | time 0[s] | loss 1.52\n",
            "| epoch 141 |  iter 1 / 2 | time 0[s] | loss 1.49\n",
            "| epoch 142 |  iter 1 / 2 | time 0[s] | loss 1.48\n",
            "| epoch 143 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
            "| epoch 144 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
            "| epoch 145 |  iter 1 / 2 | time 0[s] | loss 1.49\n",
            "| epoch 146 |  iter 1 / 2 | time 0[s] | loss 1.55\n",
            "| epoch 147 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
            "| epoch 148 |  iter 1 / 2 | time 0[s] | loss 1.49\n",
            "| epoch 149 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
            "| epoch 150 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
            "| epoch 151 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
            "| epoch 152 |  iter 1 / 2 | time 0[s] | loss 1.48\n",
            "| epoch 153 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
            "| epoch 154 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
            "| epoch 155 |  iter 1 / 2 | time 0[s] | loss 1.44\n",
            "| epoch 156 |  iter 1 / 2 | time 0[s] | loss 1.41\n",
            "| epoch 157 |  iter 1 / 2 | time 0[s] | loss 1.44\n",
            "| epoch 158 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
            "| epoch 159 |  iter 1 / 2 | time 0[s] | loss 1.44\n",
            "| epoch 160 |  iter 1 / 2 | time 0[s] | loss 1.42\n",
            "| epoch 161 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
            "| epoch 162 |  iter 1 / 2 | time 0[s] | loss 1.42\n",
            "| epoch 163 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
            "| epoch 164 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
            "| epoch 165 |  iter 1 / 2 | time 0[s] | loss 1.38\n",
            "| epoch 166 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
            "| epoch 167 |  iter 1 / 2 | time 0[s] | loss 1.37\n",
            "| epoch 168 |  iter 1 / 2 | time 0[s] | loss 1.38\n",
            "| epoch 169 |  iter 1 / 2 | time 0[s] | loss 1.44\n",
            "| epoch 170 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
            "| epoch 171 |  iter 1 / 2 | time 0[s] | loss 1.35\n",
            "| epoch 172 |  iter 1 / 2 | time 0[s] | loss 1.40\n",
            "| epoch 173 |  iter 1 / 2 | time 0[s] | loss 1.35\n",
            "| epoch 174 |  iter 1 / 2 | time 0[s] | loss 1.39\n",
            "| epoch 175 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
            "| epoch 176 |  iter 1 / 2 | time 0[s] | loss 1.42\n",
            "| epoch 177 |  iter 1 / 2 | time 0[s] | loss 1.38\n",
            "| epoch 178 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
            "| epoch 179 |  iter 1 / 2 | time 0[s] | loss 1.37\n",
            "| epoch 180 |  iter 1 / 2 | time 0[s] | loss 1.33\n",
            "| epoch 181 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
            "| epoch 182 |  iter 1 / 2 | time 0[s] | loss 1.33\n",
            "| epoch 183 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
            "| epoch 184 |  iter 1 / 2 | time 0[s] | loss 1.26\n",
            "| epoch 185 |  iter 1 / 2 | time 0[s] | loss 1.44\n",
            "| epoch 186 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
            "| epoch 187 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
            "| epoch 188 |  iter 1 / 2 | time 0[s] | loss 1.32\n",
            "| epoch 189 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
            "| epoch 190 |  iter 1 / 2 | time 0[s] | loss 1.32\n",
            "| epoch 191 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
            "| epoch 192 |  iter 1 / 2 | time 0[s] | loss 1.37\n",
            "| epoch 193 |  iter 1 / 2 | time 0[s] | loss 1.35\n",
            "| epoch 194 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 195 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
            "| epoch 196 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
            "| epoch 197 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
            "| epoch 198 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
            "| epoch 199 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
            "| epoch 200 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
            "| epoch 201 |  iter 1 / 2 | time 0[s] | loss 1.32\n",
            "| epoch 202 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
            "| epoch 203 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 204 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
            "| epoch 205 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
            "| epoch 206 |  iter 1 / 2 | time 0[s] | loss 1.41\n",
            "| epoch 207 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 208 |  iter 1 / 2 | time 0[s] | loss 1.25\n",
            "| epoch 209 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
            "| epoch 210 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 211 |  iter 1 / 2 | time 0[s] | loss 1.32\n",
            "| epoch 212 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 213 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
            "| epoch 214 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
            "| epoch 215 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
            "| epoch 216 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
            "| epoch 217 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
            "| epoch 218 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
            "| epoch 219 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 220 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
            "| epoch 221 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
            "| epoch 222 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
            "| epoch 223 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
            "| epoch 224 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
            "| epoch 225 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
            "| epoch 226 |  iter 1 / 2 | time 0[s] | loss 1.25\n",
            "| epoch 227 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 228 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
            "| epoch 229 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
            "| epoch 230 |  iter 1 / 2 | time 0[s] | loss 1.26\n",
            "| epoch 231 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
            "| epoch 232 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 233 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 234 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
            "| epoch 235 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 236 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 237 |  iter 1 / 2 | time 0[s] | loss 1.26\n",
            "| epoch 238 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
            "| epoch 239 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 240 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 241 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
            "| epoch 242 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 243 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 244 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 245 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
            "| epoch 246 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 247 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
            "| epoch 248 |  iter 1 / 2 | time 0[s] | loss 1.25\n",
            "| epoch 249 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 250 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 251 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 252 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
            "| epoch 253 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 254 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 255 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
            "| epoch 256 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 257 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
            "| epoch 258 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 259 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
            "| epoch 260 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
            "| epoch 261 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 262 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 263 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
            "| epoch 264 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 265 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 266 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
            "| epoch 267 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 268 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 269 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 270 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 271 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
            "| epoch 272 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 273 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
            "| epoch 274 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 275 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 276 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 277 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 278 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 279 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
            "| epoch 280 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
            "| epoch 281 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 282 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 283 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
            "| epoch 284 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 285 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 286 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
            "| epoch 287 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 288 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 289 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 290 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 291 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 292 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 293 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 294 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 295 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 296 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 297 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 298 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 299 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
            "| epoch 300 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 301 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 302 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 303 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 304 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 305 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
            "| epoch 306 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 307 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 308 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 309 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 310 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 311 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 312 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 313 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 314 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 315 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 316 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 317 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 318 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 319 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 320 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
            "| epoch 321 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 322 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 323 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
            "| epoch 324 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 325 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
            "| epoch 326 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 327 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 328 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 329 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 330 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 331 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 332 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 333 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 334 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 335 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 336 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 337 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 338 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 339 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 340 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 341 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 342 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 343 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 344 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 345 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 346 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
            "| epoch 347 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 348 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 349 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 350 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 351 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
            "| epoch 352 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 353 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 354 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 355 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 356 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 357 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 358 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
            "| epoch 359 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 360 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 361 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 362 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 363 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 364 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 365 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
            "| epoch 366 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 367 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 368 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 369 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 370 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 371 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 372 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 373 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 374 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 375 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 376 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 377 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 378 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 379 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 380 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
            "| epoch 381 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 382 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
            "| epoch 383 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 384 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 385 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 386 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
            "| epoch 387 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 388 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 389 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 390 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 391 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 392 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 393 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 394 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 395 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 396 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 397 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
            "| epoch 398 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 399 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 400 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
            "| epoch 401 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 402 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 403 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 404 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 405 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 406 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 407 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 408 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 409 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 410 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 411 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 412 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
            "| epoch 413 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 414 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 415 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 416 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 417 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 418 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 419 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 420 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 421 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 422 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 423 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 424 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 425 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 426 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 427 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 428 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 429 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 430 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 431 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 432 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 433 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 434 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 435 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
            "| epoch 436 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 437 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 438 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 439 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 440 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 441 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 442 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 443 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 444 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 445 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 446 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
            "| epoch 447 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 448 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
            "| epoch 449 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 450 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 451 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 452 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 453 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 454 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 455 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
            "| epoch 456 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
            "| epoch 457 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 458 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 459 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
            "| epoch 460 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 461 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 462 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 463 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
            "| epoch 464 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 465 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 466 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 467 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 468 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 469 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 470 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 471 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 472 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 473 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 474 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 475 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
            "| epoch 476 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 477 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 478 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 479 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 480 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 481 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 482 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
            "| epoch 483 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 484 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 485 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 486 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 487 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 488 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 489 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
            "| epoch 490 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 491 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 492 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 493 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 494 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 495 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 496 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 497 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 498 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 499 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 500 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 501 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 502 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 503 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 504 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
            "| epoch 505 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 506 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 507 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 508 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 509 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 510 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 511 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 512 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 513 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 514 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 515 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 516 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 517 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 518 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 519 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 520 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 521 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 522 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 523 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 524 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 525 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 526 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
            "| epoch 527 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 528 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 529 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 530 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 531 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 532 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 533 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 534 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 535 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 536 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 537 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 538 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 539 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 540 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 541 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 542 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 543 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 544 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 545 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 546 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 547 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 548 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 549 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 550 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 551 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 552 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 553 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 554 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 555 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 556 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 557 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 558 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
            "| epoch 559 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 560 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 561 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 562 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 563 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 564 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 565 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 566 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 567 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 568 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 569 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 570 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 571 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 572 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 573 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 574 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 575 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 576 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 577 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 578 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 579 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 580 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 581 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 582 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
            "| epoch 583 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 584 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 585 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 586 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 587 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 588 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 589 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 590 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 591 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 592 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 593 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 594 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 595 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 596 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 597 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 598 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 599 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 600 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 601 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 602 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 603 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
            "| epoch 604 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 605 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 606 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 607 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 608 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 609 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 610 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 611 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 612 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 613 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 614 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 615 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 616 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 617 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 618 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 619 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 620 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 621 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 622 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 623 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 624 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 625 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 626 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 627 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 628 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 629 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 630 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 631 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 632 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 633 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 634 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 635 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 636 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 637 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 638 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 639 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 640 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 641 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 642 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 643 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 644 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 645 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 646 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 647 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 648 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 649 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 650 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 651 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 652 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 653 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 654 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 655 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 656 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 657 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 658 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 659 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 660 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 661 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 662 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 663 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 664 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 665 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 666 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 667 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 668 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 669 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 670 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 671 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 672 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 673 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 674 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 675 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 676 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 677 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 678 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 679 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 680 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 681 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 682 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
            "| epoch 683 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 684 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 685 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 686 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 687 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 688 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 689 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 690 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 691 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 692 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
            "| epoch 693 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
            "| epoch 694 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
            "| epoch 695 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 696 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 697 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 698 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 699 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 700 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 701 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 702 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
            "| epoch 703 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 704 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 705 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 706 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 707 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 708 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 709 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 710 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 711 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 712 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 713 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 714 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 715 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 716 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 717 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 718 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 719 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 720 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 721 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 722 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 723 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 724 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 725 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 726 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 727 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
            "| epoch 728 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 729 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 730 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 731 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 732 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 733 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
            "| epoch 734 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 735 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 736 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
            "| epoch 737 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 738 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 739 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 740 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 741 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 742 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 743 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 744 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 745 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 746 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 747 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 748 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 749 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 750 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 751 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 752 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
            "| epoch 753 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 754 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
            "| epoch 755 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 756 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 757 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 758 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 759 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 760 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 761 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
            "| epoch 762 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 763 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 764 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 765 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 766 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 767 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 768 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
            "| epoch 769 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 770 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 771 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 772 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 773 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 774 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 775 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 776 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 777 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 778 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 779 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
            "| epoch 780 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 781 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 782 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 783 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 784 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 785 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 786 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 787 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 788 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 789 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 790 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 791 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 792 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 793 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 794 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 795 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 796 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 797 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 798 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 799 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 800 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 801 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 802 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
            "| epoch 803 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 804 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 805 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 806 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 807 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 808 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 809 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 810 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 811 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 812 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 813 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 814 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 815 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 816 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
            "| epoch 817 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 818 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
            "| epoch 819 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 820 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 821 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 822 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 823 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 824 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 825 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 826 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 827 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 828 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 829 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 830 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 831 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 832 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 833 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 834 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
            "| epoch 835 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 836 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 837 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 838 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
            "| epoch 839 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 840 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 841 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 842 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 843 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
            "| epoch 844 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
            "| epoch 845 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 846 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 847 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
            "| epoch 848 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 849 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 850 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 851 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 852 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
            "| epoch 853 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 854 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 855 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 856 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 857 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 858 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 859 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 860 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 861 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 862 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
            "| epoch 863 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 864 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
            "| epoch 865 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 866 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 867 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 868 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 869 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 870 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 871 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 872 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
            "| epoch 873 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 874 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 875 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 876 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
            "| epoch 877 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 878 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 879 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 880 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 881 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 882 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 883 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 884 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 885 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 886 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 887 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 888 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 889 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
            "| epoch 890 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 891 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 892 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 893 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
            "| epoch 894 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 895 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 896 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 897 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 898 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 899 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
            "| epoch 900 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 901 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 902 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 903 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 904 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 905 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 906 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
            "| epoch 907 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 908 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
            "| epoch 909 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 910 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 911 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 912 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 913 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 914 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 915 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 916 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 917 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 918 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
            "| epoch 919 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 920 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 921 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
            "| epoch 922 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 923 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 924 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
            "| epoch 925 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 926 |  iter 1 / 2 | time 1[s] | loss 0.77\n",
            "| epoch 927 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
            "| epoch 928 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
            "| epoch 929 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
            "| epoch 930 |  iter 1 / 2 | time 1[s] | loss 0.86\n",
            "| epoch 931 |  iter 1 / 2 | time 1[s] | loss 0.99\n",
            "| epoch 932 |  iter 1 / 2 | time 1[s] | loss 0.86\n",
            "| epoch 933 |  iter 1 / 2 | time 1[s] | loss 0.72\n",
            "| epoch 934 |  iter 1 / 2 | time 1[s] | loss 0.86\n",
            "| epoch 935 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
            "| epoch 936 |  iter 1 / 2 | time 1[s] | loss 0.71\n",
            "| epoch 937 |  iter 1 / 2 | time 1[s] | loss 1.14\n",
            "| epoch 938 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
            "| epoch 939 |  iter 1 / 2 | time 1[s] | loss 0.72\n",
            "| epoch 940 |  iter 1 / 2 | time 1[s] | loss 0.73\n",
            "| epoch 941 |  iter 1 / 2 | time 1[s] | loss 0.99\n",
            "| epoch 942 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
            "| epoch 943 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
            "| epoch 944 |  iter 1 / 2 | time 1[s] | loss 0.70\n",
            "| epoch 945 |  iter 1 / 2 | time 1[s] | loss 1.13\n",
            "| epoch 946 |  iter 1 / 2 | time 1[s] | loss 0.58\n",
            "| epoch 947 |  iter 1 / 2 | time 1[s] | loss 1.12\n",
            "| epoch 948 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
            "| epoch 949 |  iter 1 / 2 | time 1[s] | loss 0.76\n",
            "| epoch 950 |  iter 1 / 2 | time 1[s] | loss 0.70\n",
            "| epoch 951 |  iter 1 / 2 | time 1[s] | loss 1.00\n",
            "| epoch 952 |  iter 1 / 2 | time 1[s] | loss 0.83\n",
            "| epoch 953 |  iter 1 / 2 | time 1[s] | loss 0.89\n",
            "| epoch 954 |  iter 1 / 2 | time 1[s] | loss 0.83\n",
            "| epoch 955 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
            "| epoch 956 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
            "| epoch 957 |  iter 1 / 2 | time 1[s] | loss 0.86\n",
            "| epoch 958 |  iter 1 / 2 | time 1[s] | loss 0.70\n",
            "| epoch 959 |  iter 1 / 2 | time 1[s] | loss 1.00\n",
            "| epoch 960 |  iter 1 / 2 | time 1[s] | loss 0.73\n",
            "| epoch 961 |  iter 1 / 2 | time 1[s] | loss 0.94\n",
            "| epoch 962 |  iter 1 / 2 | time 1[s] | loss 0.86\n",
            "| epoch 963 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
            "| epoch 964 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
            "| epoch 965 |  iter 1 / 2 | time 1[s] | loss 0.70\n",
            "| epoch 966 |  iter 1 / 2 | time 1[s] | loss 0.97\n",
            "| epoch 967 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
            "| epoch 968 |  iter 1 / 2 | time 1[s] | loss 0.73\n",
            "| epoch 969 |  iter 1 / 2 | time 1[s] | loss 0.98\n",
            "| epoch 970 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
            "| epoch 971 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
            "| epoch 972 |  iter 1 / 2 | time 1[s] | loss 0.94\n",
            "| epoch 973 |  iter 1 / 2 | time 1[s] | loss 0.60\n",
            "| epoch 974 |  iter 1 / 2 | time 1[s] | loss 0.99\n",
            "| epoch 975 |  iter 1 / 2 | time 1[s] | loss 0.70\n",
            "| epoch 976 |  iter 1 / 2 | time 1[s] | loss 0.97\n",
            "| epoch 977 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
            "| epoch 978 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
            "| epoch 979 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
            "| epoch 980 |  iter 1 / 2 | time 1[s] | loss 0.69\n",
            "| epoch 981 |  iter 1 / 2 | time 1[s] | loss 1.00\n",
            "| epoch 982 |  iter 1 / 2 | time 1[s] | loss 0.70\n",
            "| epoch 983 |  iter 1 / 2 | time 1[s] | loss 0.96\n",
            "| epoch 984 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
            "| epoch 985 |  iter 1 / 2 | time 1[s] | loss 0.72\n",
            "| epoch 986 |  iter 1 / 2 | time 1[s] | loss 0.97\n",
            "| epoch 987 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
            "| epoch 988 |  iter 1 / 2 | time 1[s] | loss 0.99\n",
            "| epoch 989 |  iter 1 / 2 | time 1[s] | loss 0.58\n",
            "| epoch 990 |  iter 1 / 2 | time 1[s] | loss 0.99\n",
            "| epoch 991 |  iter 1 / 2 | time 1[s] | loss 0.93\n",
            "| epoch 992 |  iter 1 / 2 | time 1[s] | loss 0.75\n",
            "| epoch 993 |  iter 1 / 2 | time 1[s] | loss 0.96\n",
            "| epoch 994 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
            "| epoch 995 |  iter 1 / 2 | time 1[s] | loss 0.69\n",
            "| epoch 996 |  iter 1 / 2 | time 1[s] | loss 0.99\n",
            "| epoch 997 |  iter 1 / 2 | time 1[s] | loss 0.72\n",
            "| epoch 998 |  iter 1 / 2 | time 1[s] | loss 0.69\n",
            "| epoch 999 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
            "| epoch 1000 |  iter 1 / 2 | time 1[s] | loss 1.08\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vecs = model.word_vecs\n",
        "for word_id, word in id_to_word.items():\n",
        "    print(word, word_vecs[word_id])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdMoV3FNroIH",
        "outputId": "dae17160-40f2-4db9-e042-73b71391f4c3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you [-1.161 -1.163 -1.207 -1.204 -1.181]\n",
            "say [1.097 1.168 1.169 1.165 1.18 ]\n",
            "goodbye [-0.792 -0.746 -0.719 -0.754 -0.744]\n",
            "and [1.515 1.134 0.93  0.983 0.922]\n",
            "i [-0.797 -0.752 -0.725 -0.756 -0.767]\n",
            "hello [-1.171 -1.153 -1.225 -1.189 -1.173]\n",
            ". [-0.149  0.798  1.137  1.101  1.154]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4 word2vec の高速化"
      ],
      "metadata": {
        "id": "e5FwetdjmGWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingDot:\n",
        "    def __init__(self, W):\n",
        "        self.embed = Embedding(W)\n",
        "        self.params = self.embed.params\n",
        "        self.grads = self.embed.grads\n",
        "        self.cache = None\n",
        "    \n",
        "    def forward(self, h, idx):\n",
        "        target_W = self.embed.forward(idx)\n",
        "        out = np.sum(target_W * h, axis=1)\n",
        "        self.cache = (h, target_W)\n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        h, target_W = self.cache\n",
        "        dout = dout.reshape(dout.shape[0], 1)\n",
        "        dtarget_W = dout * h\n",
        "        self.embed.backward(dtarget_W)\n",
        "        dh = dout * target_W\n",
        "        return dh"
      ],
      "metadata": {
        "id": "wQCCo05uUZb6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnigramSampler:\n",
        "    def __init__(self, corpus, power, sample_size):\n",
        "        self.sample_size = sample_size\n",
        "        self.vocab_size = None\n",
        "        self.word_p = None\n",
        "\n",
        "        counts = collections.Counter()\n",
        "        for word_id in corpus:\n",
        "            # print(type(word_id))\n",
        "            # counts[word_id] += 1\n",
        "            counts[int(word_id)] += 1\n",
        "\n",
        "        vocab_size = len(counts)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.word_p = np.zeros(vocab_size)\n",
        "        for i in range(vocab_size):\n",
        "            self.word_p[i] = counts[i]\n",
        "\n",
        "        self.word_p = np.power(self.word_p, power)\n",
        "        self.word_p /= np.sum(self.word_p)\n",
        "\n",
        "    def get_negative_sample(self, target):\n",
        "        batch_size = target.shape[0]\n",
        "\n",
        "        if not GPU:\n",
        "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                p = self.word_p.copy()\n",
        "                target_idx = target[i]\n",
        "                p[target_idx] = 0\n",
        "                p /= p.sum()\n",
        "                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
        "        else:\n",
        "            # GPU(cupy）で計算するときは、速度を優先\n",
        "            # 負例にターゲットが含まれるケースがある\n",
        "            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
        "                                               replace=True, p=self.word_p)\n",
        "\n",
        "        return negative_sample"
      ],
      "metadata": {
        "id": "BkjfmVuFmrjt"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = np.array([0,1,2,3,4,1,2,3])\n",
        "power = 0.75\n",
        "sample_size =2\n",
        "\n",
        "#cupyだと初期化でエラーが出る\n",
        "sampler = UnigramSampler(corpus, power, sample_size)\n",
        "target = np.array([1,3,0])\n",
        "negative_sample = sampler.get_negative_sample(target)\n",
        "negative_sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pKXkV8YAlmM",
        "outputId": "73820a3d-0b1c-41f5-dbb6-05cdd146a653"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 0],\n",
              "       [1, 4],\n",
              "       [1, 2]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NegativeSmaplingLoss:\n",
        "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
        "        self.sample_size = sample_size\n",
        "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
        "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size+1)]\n",
        "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size+1)]\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.embed_dot_layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "    \n",
        "    def forward(self, h, target):\n",
        "        batch_size = target.shape[0]\n",
        "        negative_sample = self.sampler.get_negative_sample(target)\n",
        "\n",
        "        #正例のforward\n",
        "        score = self.embed_dot_layers[0].forward(h, target)\n",
        "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
        "        loss = self.loss_layers[0].forward(score, correct_label)\n",
        "\n",
        "        #負例のforward\n",
        "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
        "        for i in range(self.sample_size):\n",
        "            negative_target = negative_sample[:, i]\n",
        "            score = self.embed_dot_layers[1+i].forward(h, negative_target)\n",
        "            loss += self.loss_layers[1+i].forward(score, negative_label)\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def backward(self, dout=1):\n",
        "        dh = 0\n",
        "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
        "            dscore = l0.backward(dout)\n",
        "            dh += l1.backward(dscore)\n",
        "        return dh"
      ],
      "metadata": {
        "id": "WPhTSDQiBbCA"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOW:\n",
        "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
        "        V, H = vocab_size, hidden_size\n",
        "        #重みの初期化\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype(\"f\")\n",
        "        W_out = 0.01 * np.random.randn(V, H).astype(\"f\")\n",
        "\n",
        "        #レイヤの生成\n",
        "        self.in_layers = []\n",
        "        for i in range(2 * window_size):\n",
        "            layer = Embedding(W_in)\n",
        "            self.in_layers.append(layer)\n",
        "        self.ns_loss = NegativeSmaplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
        "\n",
        "        #全ての重みと勾配を配列にまとめる\n",
        "        layers = self.in_layers + [self.ns_loss]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        #メンバ変数に単語の分散表現を設定\n",
        "        self.word_vecs = W_in\n",
        "    \n",
        "    def forward(self, contexts, target):\n",
        "        h = 0\n",
        "        for i, layer in enumerate(self.in_layers):\n",
        "            h += layer.forward(contexts[:, i])\n",
        "        h *= 1 / len(self.in_layers)\n",
        "        loss = self.ns_loss.forward(h, target)\n",
        "        return loss\n",
        "    \n",
        "    def backward(self, dout=1):\n",
        "        dout = self.ns_loss.backward(dout)\n",
        "        dout *= 1 / len(self.in_layers)\n",
        "        for layer in self.in_layers:\n",
        "            layer.backward(dout)\n",
        "        return None"
      ],
      "metadata": {
        "id": "Jp9ibVQRHcx7"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBOWの学習と評価"
      ],
      "metadata": {
        "id": "QIELWmu7Tr5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ハイパパラメータの設定\n",
        "window_size = 5\n",
        "hidden_size = 100\n",
        "batch_size = 100\n",
        "max_epoch = 10\n",
        "\n",
        "#データの読み込み\n",
        "ptb = Ptb()\n",
        "corpus, word_to_id, id_to_word = ptb.load_data(\"train\")\n",
        "vocab_size = len(word_to_id)\n",
        "\n",
        "# contexts, target = create_contexts_target(corpus, window_size)\n",
        "# if GPU:\n",
        "#     contexts, target = to_gpu(contexts), to_gpu(target)"
      ],
      "metadata": {
        "id": "26MXOu_5L8LO"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#モデルなどの生成\n",
        "# model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
        "# optimizer = Adam()\n",
        "# trainer = Trainer(model, optimizer)\n",
        "\n",
        "#学習開始\n",
        "# trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "# trainer.plot()"
      ],
      "metadata": {
        "id": "aPMEtOTXOTq_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#後程利用できるように必要なデータを保存\n",
        "# word_vecs = model.word_vecs\n",
        "# if GPU:\n",
        "#     word_vecs = to_cpu(word_vecs)\n",
        "\n",
        "# params = {}\n",
        "# params[\"word_vecs\"] = word_vecs.astype(np.float16)\n",
        "# params[\"word_to_id\"] = word_to_id\n",
        "# params[\"id_to_word\"] = id_to_word\n",
        "\n",
        "# pkl_file = \"/content/drive/MyDrive/Colab Notebooks/tmp/cbow_params.pkl\"\n",
        "# with open(pkl_file, \"wb\") as f:\n",
        "#     pickle.dump(params, f, -1)"
      ],
      "metadata": {
        "id": "yX1RqFRAFEfW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pkl_file = '/content/drive/MyDrive/Colab Notebooks/tmp/cbow_params.pkl'\n",
        "\n",
        "# with open(pkl_file, 'rb') as f:\n",
        "#     params = pickle.load(f)\n",
        "#     word_vecs = params['word_vecs']\n",
        "#     word_to_id = params['word_to_id']\n",
        "#     id_to_word = params['id_to_word']\n",
        "\n",
        "# # most similar task\n",
        "# querys = ['you', 'year', 'car', 'toyota']\n",
        "# for query in querys:\n",
        "#     most_similar(query, word_to_id, id_to_word, word_vecs, top=5)\n",
        "\n",
        "# # analogy task\n",
        "# print('-'*50)\n",
        "# analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
        "# analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
        "# analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\n",
        "# analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)"
      ],
      "metadata": {
        "id": "MgE7Mt8UPivN"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5 RNN"
      ],
      "metadata": {
        "id": "n9rW5MScdazD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRnnlm:\n",
        "    def __init__(self, vocab_size,  wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "    \n",
        "        #重みの初期化\n",
        "        embed_W = (rn(V, D) / 100).astype(\"f\")\n",
        "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype(\"f\") \n",
        "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype(\"f\")\n",
        "        rnn_b = np.zeros(H).astype(\"f\")\n",
        "        affine_W = (rn(H, V) / np.sqrt(H)).astype(\"f\") \n",
        "        affine_b = np.zeros(V).astype(\"f\")\n",
        "\n",
        "        #レイヤの生成\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n",
        "            TimeAffine(affine_W, affine_b),\n",
        "        ]\n",
        "        self.loss_layer = TimeSoftmaxWithLoss()\n",
        "        self.rnn_layer = self.layers[1]\n",
        "\n",
        "        #全ての重みと勾配をリストにまとめる\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params \n",
        "            self.grads += layer.grads\n",
        "    \n",
        "    def forward(self, xs, ts):\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "        loss = self.loss_layer.forward(xs, ts)\n",
        "        return loss\n",
        "    \n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout\n",
        "    \n",
        "    def reset_state(self):\n",
        "        self.rnn_layer.reset_state()"
      ],
      "metadata": {
        "id": "p-ZFfFwqdcRo"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #ハイパパラメータの設定\n",
        "# batch_size = 10\n",
        "# wordvec_size= 100\n",
        "# hidden_size = 100#RNNの隠れ状態ベクトルの要素数\n",
        "# time_size = 5     #Truncated BPTTの展開する時間サイズ\n",
        "# lr = 0.1\n",
        "# max_epoch = 100\n",
        "\n",
        "# #データの読み込み\n",
        "# ptb = Ptb()\n",
        "# corpus, word_to_id, id_to_word = ptb.load_data(\"train\")\n",
        "# corpus_size = 1000\n",
        "# corpus = corpus[:corpus_size]\n",
        "# vocab_size = int(max(corpus) +1)\n",
        "\n",
        "# xs = corpus[:-1]#入力\n",
        "# ts = corpus[1:] #出力：教師ラベル\n",
        "# data_size = len(xs)\n",
        "# print(f'corpus size: {corpus_size}, vocabulary size: {vocab_size}')\n",
        "\n",
        "# #学習時に使用する変数\n",
        "# max_iters = data_size // (batch_size + time_size)\n",
        "# time_idx = 0\n",
        "# total_loss = 0\n",
        "# loss_count = 0\n",
        "# ppl_list = []\n",
        "\n",
        "# #モデルの生成\n",
        "# model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
        "# optimizer = SGD(lr)\n",
        "\n",
        "# #1 ミニバッチの各サンプルの読み込み開始位置を計算\n",
        "# jump = (corpus_size -1) // batch_size\n",
        "# offsets = [i * jump for i in range(batch_size)]\n",
        "\n",
        "# for epoch in range(max_epoch):\n",
        "#     for iter in range(max_iters):\n",
        "#         #2 ミニバッチの取得\n",
        "#         batch_x = np.empty((batch_size, time_size), dtype=\"i\")\n",
        "#         batch_t = np.empty((batch_size, time_size), dtype=\"i\")\n",
        "#         for t in range(time_size):\n",
        "#             for i, offset in enumerate(offsets):\n",
        "#                 batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
        "#                 batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
        "#             time_idx += 1\n",
        "        \n",
        "#         #勾配を求め、パラメータを更新\n",
        "#         loss = model.forward(batch_x, batch_t)\n",
        "#         model.backward()\n",
        "#         optimizer.update(model.params, model.grads)\n",
        "#         total_loss += loss\n",
        "#         loss_count += 1\n",
        "    \n",
        "#     #3 epochごとにパープレキシティの評価\n",
        "#     ppl = np.exp(total_loss / loss_count)\n",
        "#     print('| epoch %d | perplexity %.2f' % (epoch+1, ppl))\n",
        "#     ppl_list.append(float(ppl))\n",
        "#     total_loss, loss_count = 0, 0"
      ],
      "metadata": {
        "id": "oxMUgGlBdyDL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(ppl_list)\n",
        "# plt.xlabel(\"epochs\")\n",
        "# plt.ylabel(\"perplexity\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Sbvgpv4vHNSP"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6 ゲート付きRNN"
      ],
      "metadata": {
        "id": "LQAlW07ROYSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 2\n",
        "H = 3\n",
        "T = 20\n",
        "\n",
        "dh = np.ones((N, H))\n",
        "np.random.seed(3)\n",
        "# Wh = np.random.randn(H, H)\n",
        "Wh = np.random.randn(H, H)*0.5\n",
        "\n",
        "norm_list = []\n",
        "for t in range(T):\n",
        "    dh = np.dot(dh, Wh.T)\n",
        "    norm = np.sqrt(np.sum(dh**2)) / N\n",
        "    norm_list.append(norm)\n",
        "\n",
        "plt.plot(norm_list)\n",
        "plt.xlabel(\"time step\")\n",
        "plt.ylabel(\"norm\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "sxJXwqUOHefg",
        "outputId": "63956034-d820-4d79-b7f3-94ad78317918"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "勾配爆発への対策：勾配クリッピング"
      ],
      "metadata": {
        "id": "w5E_mjyuSb8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dW1 = np.random.rand(3, 3)*10\n",
        "dW2 = np.random.rand(3, 3)*10\n",
        "grads = [dW1, dW2]\n",
        "max_norm = 5.0\n",
        "print(\"before\")\n",
        "print(dW1)\n",
        "\n",
        "clip_grads(grads, max_norm)\n",
        "print(\"after\")\n",
        "print(dW1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hddIbll5PcWI",
        "outputId": "a6e856b7-71e2-4acf-c902-66f30feadf9d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before\n",
            "[[6.491 2.785 6.763]\n",
            " [5.909 0.24  5.589]\n",
            " [2.593 4.151 2.835]]\n",
            "after\n",
            "[[1.495 0.641 1.557]\n",
            " [1.361 0.055 1.287]\n",
            " [0.597 0.956 0.653]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "勾配消失への対策：セル、ouputゲート、forgetゲート、inputゲートの導入"
      ],
      "metadata": {
        "id": "phUVV0P4Sni8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Rnnlm(BaseModel):\n",
        "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        # 重みの初期化\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        # レイヤの生成\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
        "            TimeAffine(affine_W, affine_b)\n",
        "        ]\n",
        "        self.loss_layer = TimeSoftmaxWithLoss()\n",
        "        self.lstm_layer = self.layers[1]\n",
        "\n",
        "        # すべての重みと勾配をリストにまとめる\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def predict(self, xs):\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "        return xs\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        score = self.predict(xs)\n",
        "        loss = self.loss_layer.forward(score, ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.lstm_layer.reset_state()"
      ],
      "metadata": {
        "id": "5xqx2J_4seFi"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #ハイパパラメータの生成\n",
        "# batch_size = 20\n",
        "# wordvec_size = 100\n",
        "# hidden_size = 100\n",
        "# time_size = 35\n",
        "# lr = 20.0\n",
        "# max_epoch = 4\n",
        "# max_grad = 0.25\n",
        "\n",
        "# #学習データの読み込み\n",
        "# ptb = Ptb()\n",
        "# corpus, word_to_id, id_to_word = ptb.load_data(\"train\")\n",
        "# corpus_test, _ , _ = ptb.load_data(\"test\")\n",
        "# vocab_size = len(word_to_id)\n",
        "# xs = corpus[:-1]#入力\n",
        "# ts = corpus[1:] #出力：教師ラベル\n",
        "\n",
        "# #モデルの生成\n",
        "# model = Rnnlm(vocab_size, wordvec_size, hidden_size)\n",
        "# optimizer = SGD(lr)\n",
        "# trainer = RnnlmTrainer(model, optimizer)\n",
        "\n",
        "# #勾配クリッピングを適用して学習\n",
        "# trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad, eval_interval=20)\n",
        "# trainer.plot(ylim=(0, 500))\n",
        "\n",
        "# #テストデータで評価\n",
        "# model.reset_state()\n",
        "# ppl_test = eval_perplexity(model, corpus_test)\n",
        "# print('test perplexity: ', ppl_test) \n",
        "\n",
        "# #パラメータの保存\n",
        "# model.save_params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "dW1XKmy7ZcjP",
        "outputId": "7d9ff535-07bd-4699-90dc-7ea00000a1ef"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch 1 |  iter 1 / 1327 | time 0[s] | perplexity 9999.69\n",
            "| epoch 1 |  iter 21 / 1327 | time 8[s] | perplexity 2965.95\n",
            "| epoch 1 |  iter 41 / 1327 | time 18[s] | perplexity 1249.61\n",
            "| epoch 1 |  iter 61 / 1327 | time 25[s] | perplexity 961.73\n",
            "| epoch 1 |  iter 81 / 1327 | time 30[s] | perplexity 808.38\n",
            "| epoch 1 |  iter 101 / 1327 | time 35[s] | perplexity 655.81\n",
            "| epoch 1 |  iter 121 / 1327 | time 41[s] | perplexity 652.26\n",
            "| epoch 1 |  iter 141 / 1327 | time 46[s] | perplexity 594.54\n",
            "| epoch 1 |  iter 161 / 1327 | time 50[s] | perplexity 577.65\n",
            "| epoch 1 |  iter 181 / 1327 | time 55[s] | perplexity 597.14\n",
            "| epoch 1 |  iter 201 / 1327 | time 60[s] | perplexity 498.29\n",
            "| epoch 1 |  iter 221 / 1327 | time 65[s] | perplexity 489.40\n",
            "| epoch 1 |  iter 241 / 1327 | time 70[s] | perplexity 452.50\n",
            "| epoch 1 |  iter 261 / 1327 | time 74[s] | perplexity 471.56\n",
            "| epoch 1 |  iter 281 / 1327 | time 79[s] | perplexity 446.90\n",
            "| epoch 1 |  iter 301 / 1327 | time 84[s] | perplexity 400.39\n",
            "| epoch 1 |  iter 321 / 1327 | time 89[s] | perplexity 344.39\n",
            "| epoch 1 |  iter 341 / 1327 | time 94[s] | perplexity 404.43\n",
            "| epoch 1 |  iter 361 / 1327 | time 99[s] | perplexity 414.39\n",
            "| epoch 1 |  iter 381 / 1327 | time 103[s] | perplexity 342.78\n",
            "| epoch 1 |  iter 401 / 1327 | time 108[s] | perplexity 354.56\n",
            "| epoch 1 |  iter 421 / 1327 | time 113[s] | perplexity 351.04\n",
            "| epoch 1 |  iter 441 / 1327 | time 118[s] | perplexity 339.07\n",
            "| epoch 1 |  iter 461 / 1327 | time 124[s] | perplexity 325.85\n",
            "| epoch 1 |  iter 481 / 1327 | time 128[s] | perplexity 310.95\n",
            "| epoch 1 |  iter 501 / 1327 | time 133[s] | perplexity 317.08\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-c5d4b8dbb795>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#勾配クリッピングを適用して学習\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-eb6e8553520e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, xs, ts, max_epoch, batch_size, time_size, max_grad, eval_interval)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;31m# 勾配を求め、パラメータを更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 共有された重みを1つに集約\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmax_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-6febb74c6822>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dout)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-941ab39334eb>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dout)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BetterRnnlm(BaseModel):\n",
        "    def __init__(self, vocab_size=10000, wordvec_size=650, hidden_size=650, dropout_ratio=0.5):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype(\"f\")\n",
        "        lstm_Wx1 = (rn(D, 4*H) / np.sqrt(D)).astype(\"f\")\n",
        "        lstm_Wh1 = (rn(H, 4*H) / np.sqrt(H)).astype(\"f\")\n",
        "        lstm_b1 = np.zeros(4*H).astype(\"f\")\n",
        "        lstm_Wx2 = (rn(H, 4*H) / np.sqrt(H)).astype(\"f\")\n",
        "        lstm_Wh2 = (rn(H, 4*H) / np.sqrt(H)).astype(\"f\")\n",
        "        lstm_b2 = np.zeros(4*H).astype(\"f\")\n",
        "        affine_b = np.zeros(V).astype(\"f\")\n",
        "\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeDropout(dropout_ratio),\n",
        "            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),\n",
        "            TimeDropout(dropout_ratio),\n",
        "            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),\n",
        "            TimeDropout(dropout_ratio),\n",
        "            TimeAffine(embed_W.T, affine_b)#重み共有\n",
        "        ]\n",
        "        self.loss_layer = TimeSoftmaxWithLoss()\n",
        "        self.lstm_layers = [self.layers[2], self.layers[4]]\n",
        "        self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "        \n",
        "    def predict(self, xs, train_flg=False):\n",
        "        for layer in self.drop_layers:\n",
        "            layer.train_flg = train_flg\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "        return xs\n",
        "    \n",
        "    def forward(self, xs, ts, train_flg=True):\n",
        "        score = self.predict(xs, train_flg)\n",
        "        loss = self.loss_layer.forward(score)\n",
        "        return loss\n",
        "    \n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout\n",
        "    \n",
        "    def reset_state(self):\n",
        "        for layer in self.lstm_layers:\n",
        "            layer.reset_state()"
      ],
      "metadata": {
        "id": "VLWXVxlLlBzp"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ハイパーパラメータの設定\n",
        "# batch_size = 20\n",
        "# wordvec_size = 650 \n",
        "# hidden_size = 650\n",
        "# time_size = 35\n",
        "# lr = 20.0 \n",
        "# max_epoch = 40 \n",
        "# max_grad = 0.25 \n",
        "# dropout = 0.5 \n",
        "\n",
        "# # 学習データの読み込み \n",
        "# corpus, word_to_id, id_to_word = ptb.load_data('train') \n",
        "# corpus_val, _, _ = ptb.load_data('val') \n",
        "# corpus_test, _, _ = ptb.load_data('test') \n",
        "\n",
        "# vocab_size = len(word_to_id) \n",
        "# xs = corpus[:-1] \n",
        "# ts = corpus[1:] \n",
        "\n",
        "# model = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout) \n",
        "# optimizer = SGD(lr) \n",
        "# trainer = RnnlmTrainer(model, optimizer)\n",
        "\n",
        "# best_ppl = float(\"inf\")\n",
        "# for epoch in range(max_epoch):\n",
        "#     trainer.fit(xs, ts, max_epoch=1, batch_size=batch_size, time_size=time_size, max_grad=max_grad)\n",
        "#     model.reset_state()\n",
        "#     ppl = eval_perplexity(model, corpus_val)\n",
        "#     print('valid perplexity: ', ppl)\n",
        "\n",
        "#     if best_ppl > ppl:\n",
        "#         best_ppl = ppl\n",
        "#         model.save_params(\"/content/drive/MyDrive/Colab Notebooks/tmp/Rnnlm.pkl\")\n",
        "#     else:\n",
        "#         lr /= 4.0\n",
        "#         optimizer.lr = lr\n",
        "\n",
        "#     model.reset_state()\n",
        "#     print(\"-\" *50)"
      ],
      "metadata": {
        "id": "84NuyUphvQ0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7 RNNによる文章生成"
      ],
      "metadata": {
        "id": "BRbuP63_19zF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RnnlmGen(Rnnlm):\n",
        "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
        "        word_ids = [start_id]\n",
        "\n",
        "        x = start_id\n",
        "        while len(word_ids) < sample_size:\n",
        "            x = np.array(x).reshape(1, 1)\n",
        "            score = self.predict(x)\n",
        "            p = softmax(score.flatten())\n",
        "\n",
        "            sampled = np.random.choice(len(p), size=1, p=p)\n",
        "            if (skip_ids is None) or (sampled not in skip_ids):\n",
        "                x = sampled\n",
        "                word_ids.append(int(x))\n",
        "        return word_ids"
      ],
      "metadata": {
        "id": "ooVGEOyb2Aou"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ptb = Ptb()\n",
        "corpus, word_to_id, id_to_word = ptb.load_data(\"train\")\n",
        "vocab_size = len(word_to_id)\n",
        "corpus_size = len(corpus)\n",
        "\n",
        "model = RnnlmGen()\n",
        "model.load_params(\"/content/drive/MyDrive/Colab Notebooks/tmp/Rnnlm.pkl\")\n",
        "\n",
        "#start文字とskip文字の設定\n",
        "start_word = \"you\"\n",
        "start_id = word_to_id[start_word]\n",
        "skip_words = [\"N\", \"<unk>\", \"$\"]\n",
        "skip_ids = [word_to_id[w] for w in skip_words]\n",
        "\n",
        "#文章生成\n",
        "word_ids = model.generate(start_id, skip_ids)\n",
        "txt = \" \".join([id_to_word[i] for i in word_ids])\n",
        "txt = txt.replace(\" <eos>\", \".\\n\")\n",
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uipTe1r7Waqa",
        "outputId": "d70fd028-9884-4af0-f354-787d2b5d9d39"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you caution.\n",
            " it was the mess of an portrayal of employees with the planet.\n",
            " the rothschilds always be this told the maker of negative accompanied parents has n't taken yet full-time projects.\n",
            " but the stock would be wrap and loss over the catalyst was.\n",
            " the capital brand wall has a standard of else this to serve.\n",
            " in that market we own your technique could identify people or what 's no gloomy tasks could be able to sell a way between sure of double the basis of his partnership 's stake.\n",
            " no work refused to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BetterRnnlmGen(BetterRnnlm):\n",
        "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
        "        word_ids = [start_id]\n",
        "\n",
        "        x = start_id\n",
        "        while len(word_ids) < sample_size:\n",
        "            x = np.array(x).reshape(1, 1)\n",
        "            score = self.predict(x)\n",
        "            p = softmax(score.flatten())\n",
        "\n",
        "            sampled = np.random.choice(len(p), size=1, p=p)\n",
        "            if (skip_ids is None) or (sampled not in skip_ids):\n",
        "                x = sampled\n",
        "                word_ids.append(int(x))\n",
        "        return word_ids"
      ],
      "metadata": {
        "id": "zBMf0m4AXsz_"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ptb = Ptb()\n",
        "corpus, word_to_id, id_to_word = ptb.load_data(\"train\")\n",
        "vocab_size = len(word_to_id)\n",
        "corpus_size = len(corpus)\n",
        "\n",
        "model = BetterRnnlmGen()\n",
        "model.load_params(\"/content/drive/MyDrive/Colab Notebooks/tmp/BetterRnnlm.pkl\")\n",
        "\n",
        "#start文字とskip文字の設定\n",
        "start_word = \"you\"\n",
        "start_id = word_to_id[start_word]\n",
        "skip_words = [\"N\", \"<unk>\", \"$\"]\n",
        "skip_ids = [word_to_id[w] for w in skip_words]\n",
        "\n",
        "#文章生成\n",
        "word_ids = model.generate(start_id, skip_ids)\n",
        "txt = \" \".join([id_to_word[i] for i in word_ids])\n",
        "txt = txt.replace(\" <eos>\", \".\\n\")\n",
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfsw9grjZGzW",
        "outputId": "f90a6d39-54ef-4895-a5be-544a9bdcb48f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you wish to be making this move.\n",
            " the plan expired in july is to office an appeal from holiday further than thursday 's agreement.\n",
            " still even the company faces loan the outcome of being filed in march.\n",
            " a halt on the stock exchange 's high sank greatly by the uncertainty in what do you perceived any sample of program trading.\n",
            " enemies in the market were only a very good indicator for companies.\n",
            " as the relationship limits the dollar sell-off a little more and ride confusion about the s&p futures.\n",
            " you did very even if\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GWznFvCpZsi3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}